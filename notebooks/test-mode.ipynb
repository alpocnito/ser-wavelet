{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc6b0040",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f145760c9d0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(\n",
    "                  os.path.dirname(\"test-mode1\"), \n",
    "                  os.pardir)\n",
    ")\n",
    "sys.path.append(PROJECT_ROOT)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import json\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from src.dataset import create_dataloader\n",
    "from src.utils import feature_extraction_pipeline, read_features_files, choose_model, read_feature\n",
    "from src.data_augmentation import Mixup, Cutmix, Specmix\n",
    "from src.models.utils import SaveBestModel, weight_init\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from typing import Dict, Tuple, List, Union\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# making sure the experiments are reproducible\n",
    "seed = 2109\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "def seed_worker(worker_id: int):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "g = torch.Generator()\n",
    "g.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1c6cba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model: nn.Module,\n",
    "    dataloader: DataLoader,\n",
    "    optimizer: torch.optim.Adam,\n",
    "    loss: torch.nn.CrossEntropyLoss,\n",
    "    device: torch.device,\n",
    "    mixer: Union[None, Mixup, Specmix, Cutmix]\n",
    ") -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Function responsible for the model training.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): the created model.\n",
    "        dataloader (DataLoader): the training dataloader.\n",
    "        optimizer (torch.optim.Adam): the optimizer used.\n",
    "        loss (torch.nn.CrossEntropyLoss): the loss function used.\n",
    "        device (torch.device): which device to use.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[float, float]: the training f1 and loss, respectively.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    predictions = []\n",
    "    targets = []\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    for index, (batch) in enumerate(dataloader, start=1):\n",
    "        data = batch[\"features\"].to(device)\n",
    "        target = batch[\"labels\"].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        data = data.to(dtype=torch.float32)\n",
    "        target = target.to(dtype=torch.float32)\n",
    "        \n",
    "        if not mixer is None:\n",
    "            data, target = mixer(\n",
    "                x=data,\n",
    "                y=target\n",
    "            )\n",
    "            \n",
    "        output = model(data)\n",
    "\n",
    "        l = loss(output, target)\n",
    "        train_loss += l.item()\n",
    "        \n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        prediction = output.argmax(dim=-1, keepdim=True).to(dtype=torch.int)\n",
    "        prediction = prediction.detach().cpu().numpy()\n",
    "        predictions.extend(prediction.tolist())\n",
    "        \n",
    "        target = target.argmax(dim=-1, keepdim=True).to(dtype=torch.int)\n",
    "        target = target.detach().cpu().numpy()\n",
    "        targets.extend(target.tolist())\n",
    "        \n",
    "    train_loss = train_loss/index\n",
    "    train_f1 = classification_report(\n",
    "        targets,\n",
    "        predictions,\n",
    "        digits=6,\n",
    "        output_dict=True,\n",
    "        zero_division=0.0\n",
    "    )\n",
    "    train_f1 = train_f1[\"macro avg\"][\"f1-score\"]\n",
    "    return train_f1, train_loss\n",
    "\n",
    "def evaluate(\n",
    "    model: nn.Module,\n",
    "    dataloader: DataLoader,\n",
    "    loss: torch.nn.CrossEntropyLoss,\n",
    "    device: torch.device\n",
    ") -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Function responsible for the model evaluation.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): the created model.\n",
    "        dataloader (DataLoader): the validaiton dataloader.\n",
    "        loss (torch.nn.CrossEntropyLoss): the loss function used.\n",
    "        device (torch.device): which device to use.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[float, float]: the validation f1 and loss, respectively.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    targets = []\n",
    "    validation_loss = 0.0\n",
    "    validation_f1 = []\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        for index, (batch) in enumerate(dataloader):\n",
    "            data = batch[\"features\"].to(device)\n",
    "            target = batch[\"labels\"].to(device)\n",
    "\n",
    "            data = data.to(dtype=torch.float32)\n",
    "            target = target.to(dtype=torch.float32)\n",
    "                        \n",
    "            output = model(data)\n",
    "            \n",
    "            l = loss(output, target)\n",
    "            validation_loss += l.item()\n",
    "            \n",
    "            prediction = output.argmax(dim=-1, keepdim=True).to(dtype=torch.int)\n",
    "            prediction = prediction.detach().cpu().numpy()\n",
    "            predictions.extend(prediction.tolist())\n",
    "            \n",
    "            target = target.argmax(dim=-1, keepdim=True).to(dtype=torch.int)\n",
    "            target = target.detach().cpu().numpy()\n",
    "            targets.extend(target.tolist())\n",
    "    \n",
    "    validation_loss = validation_loss/index\n",
    "    validation_f1 = classification_report(\n",
    "        targets,\n",
    "        predictions,\n",
    "        digits=6,\n",
    "        output_dict=True,\n",
    "        zero_division=0.0\n",
    "    )\n",
    "    validation_f1 = validation_f1[\"macro avg\"][\"f1-score\"]\n",
    "    return validation_f1, validation_loss\n",
    "\n",
    "def test(\n",
    "    model: nn.Module,\n",
    "    dataloader: DataLoader,\n",
    "    device: torch.device\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Function responsible for the model testing in the test dataset.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): the created model.\n",
    "        dataloader (DataLoader): the test dataloader.\n",
    "        device (torch.device): which device to use.\n",
    "\n",
    "    Returns:\n",
    "        str: the test classification report.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    targets = []\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        for batch in dataloader:\n",
    "            data = batch[\"features\"].to(device)\n",
    "            target = batch[\"labels\"].to(device)\n",
    "\n",
    "            data = data.to(dtype=torch.float32)\n",
    "            target = target.to(dtype=torch.float32)\n",
    "            \n",
    "            output = model(data)\n",
    "            \n",
    "            prediction = output.argmax(dim=-1, keepdim=True).to(dtype=torch.int)\n",
    "            prediction = prediction.detach().cpu().numpy()\n",
    "            predictions.extend(prediction.tolist())\n",
    "            \n",
    "            target = target.argmax(dim=-1, keepdim=True).to(dtype=torch.int)\n",
    "            target = target.detach().cpu().numpy()\n",
    "            targets.extend(target.tolist())\n",
    "    \n",
    "    class_report = classification_report(\n",
    "        targets,\n",
    "        predictions,\n",
    "        digits=4,\n",
    "        output_dict=True\n",
    "    )\n",
    "    return class_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bde39444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: torch.Size([500, 1, 128000]), torch.Size([500, 3])\n",
      "Valid: torch.Size([125, 1, 128000]), torch.Size([125, 3])\n",
      "Test: torch.Size([308, 1, 128000]), torch.Size([308, 3])\n"
     ]
    }
   ],
   "source": [
    "features_path = \"../features8k/propor2022/\"\n",
    "\n",
    "# loading training features\n",
    "X_train = read_feature(path=features_path, fold=\"0\", name=\"X_train.pth\")\n",
    "y_train = read_feature(path=features_path, fold=\"0\", name=\"y_train.pth\")\n",
    "print(f\"Train: {X_train.shape}, {y_train.shape}\")\n",
    "\n",
    "# loading validation features\n",
    "X_valid = read_feature(path=features_path, fold=\"0\", name=\"X_valid.pth\")\n",
    "y_valid = read_feature(path=features_path, fold=\"0\", name=\"y_valid.pth\")\n",
    "print(f\"Valid: {X_valid.shape}, {y_valid.shape}\")\n",
    "\n",
    "# loading testing features\n",
    "X_test = read_feature(path=features_path, fold=None, name=\"X_test.pth\")\n",
    "y_test = read_feature(path=features_path, fold=None, name=\"y_test.pth\")\n",
    "print(f\"Test: {X_test.shape}, {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce05f0e",
   "metadata": {},
   "source": [
    "## CNN 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b07bdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the parameters configuration file\n",
    "params = json.load(open(\"../config/mode_1.json\", \"r\"))\n",
    "\n",
    "# parameters defination\n",
    "k_fold = None\n",
    "max_seconds = 16\n",
    "\n",
    "if \"kfold\" in params.keys():\n",
    "    k_fold = params[\"kfold\"][\"num_k\"]\n",
    "\n",
    "max_samples = max_seconds * int(params[\"sample_rate\"])\n",
    "\n",
    "feature_config = params[\"feature\"]\n",
    "feature_config[\"sample_rate\"] = int(params[\"sample_rate\"])\n",
    "data_augmentation_config = params[\"data_augmentation\"]\n",
    "dataset = params[\"dataset\"]\n",
    "wavelet_config = params[\"wavelet\"]\n",
    "\n",
    "model_config = params[\"model\"]\n",
    "model_config[\"name\"] = \"cnn3\"\n",
    "model_config[\"use_gpu\"] = False\n",
    "\n",
    "mode = params[\"mode\"]\n",
    "\n",
    "feat_path = os.path.join(params[\"output_path\"], params[\"dataset\"])\n",
    "\n",
    "if dataset == \"propor2022\":\n",
    "    if data_augmentation_config[\"target\"] == \"majority\":\n",
    "        data_augment_target = [0]\n",
    "    elif data_augmentation_config[\"target\"] == \"minority\":\n",
    "        data_augment_target = [1, 2]\n",
    "    elif data_augmentation_config[\"target\"] == \"all\":\n",
    "        data_augment_target = [0, 1, 2]\n",
    "    else:\n",
    "        raise ValueError(\"Invalid arguments for target. Should be 'all', 'majority' or 'minority\")\n",
    "else:\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28a3df5d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/greca/HD/anaconda3/envs/ser/lib/python3.8/site-packages/librosa/util/decorators.py:88: UserWarning: Empty filters detected in mel frequency basis. Some channels will produce empty responses. Try increasing your sampling rate (and fmax) or reducing n_mels.\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/100\n",
      "\n",
      "Epoch: 1\n",
      "Train F1-Score: 0.319296\n",
      "Train Loss: 1.880261\n",
      "Validation F1-Score: 0.331320\n",
      "Validation Loss: 5.613499\n",
      "Test F1-Score: 0.340035\n",
      "\n",
      "Epoch: 2/100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 110\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, model_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 110\u001b[0m     train_f1, train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmixer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmixer\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m     valid_f1, valid_loss \u001b[38;5;241m=\u001b[39m evaluate(\n\u001b[1;32m    120\u001b[0m         device\u001b[38;5;241m=\u001b[39mdevice,\n\u001b[1;32m    121\u001b[0m         dataloader\u001b[38;5;241m=\u001b[39mvalidation_dataloader,\n\u001b[1;32m    122\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    123\u001b[0m         loss\u001b[38;5;241m=\u001b[39mloss\n\u001b[1;32m    124\u001b[0m     )\n\u001b[1;32m    126\u001b[0m     report \u001b[38;5;241m=\u001b[39m test(\n\u001b[1;32m    127\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    128\u001b[0m         dataloader\u001b[38;5;241m=\u001b[39mtest_dataloader,\n\u001b[1;32m    129\u001b[0m         device\u001b[38;5;241m=\u001b[39mdevice\n\u001b[1;32m    130\u001b[0m     )\n",
      "Cell \u001b[0;32mIn[4], line 41\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, dataloader, optimizer, loss, device, mixer)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mixer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     36\u001b[0m     data, target \u001b[38;5;241m=\u001b[39m mixer(\n\u001b[1;32m     37\u001b[0m         x\u001b[38;5;241m=\u001b[39mdata,\n\u001b[1;32m     38\u001b[0m         y\u001b[38;5;241m=\u001b[39mtarget\n\u001b[1;32m     39\u001b[0m     )\n\u001b[0;32m---> 41\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m l \u001b[38;5;241m=\u001b[39m loss(output, target)\n\u001b[1;32m     44\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m l\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m/media/greca/HD/anaconda3/envs/ser/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/media/greca/HD/GitHub/ser-wavelet/src/models/cnn3.py:216\u001b[0m, in \u001b[0;36mTransfer_CNN10.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Input: (batch_size, data_length)\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m     output_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m     embedding \u001b[38;5;241m=\u001b[39m output_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124membedding\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    218\u001b[0m     clipwise_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc_transfer(embedding)\n",
      "File \u001b[0;32m/media/greca/HD/anaconda3/envs/ser/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/media/greca/HD/GitHub/ser-wavelet/src/models/cnn3.py:161\u001b[0m, in \u001b[0;36mCNN10.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28minput\u001b[39m: torch\u001b[38;5;241m.\u001b[39mTensor\n\u001b[1;32m    159\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    160\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_block1(\u001b[38;5;28minput\u001b[39m, pool_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m), pool_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mavg\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 161\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_block2(x, pool_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m), pool_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mavg\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    163\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mdropout(x, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m)\n",
      "File \u001b[0;32m/media/greca/HD/anaconda3/envs/ser/lib/python3.8/site-packages/torch/nn/functional.py:1252\u001b[0m, in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m   1250\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n\u001b[1;32m   1251\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout probability has to be between 0 and 1, \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(p))\n\u001b[0;32m-> 1252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _VF\u001b[38;5;241m.\u001b[39mdropout_(\u001b[38;5;28minput\u001b[39m, p, training) \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# creating and defining the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available and model_config[\"use_gpu\"] else \"cpu\")\n",
    "\n",
    "model = choose_model(\n",
    "    mode=mode,\n",
    "    model_name=model_config[\"name\"],\n",
    "    device=device\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    params=model.parameters(),\n",
    "    lr=model_config[\"learning_rate\"],\n",
    "    betas=(0.9, 0.98),\n",
    "    eps=1e-9,\n",
    "    weight_decay=0\n",
    ")\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "scheduler = None\n",
    "mixer = None\n",
    "\n",
    "# creating the model checkpoint object\n",
    "sbm = SaveBestModel(\n",
    "    output_dir=os.path.join(model_config[\"output_path\"], dataset, mode, model_config[\"name\"]),\n",
    "    model_name=model_config[\"name\"]\n",
    ")\n",
    "\n",
    "if model_config[\"use_lr_scheduler\"]:\n",
    "    print(\"\\nWARNING: Using learning rate scheduler!\\n\")\n",
    "    scheduler = StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "if \"mixup\" in data_augmentation_config[\"techniques\"].keys():\n",
    "    print(\"\\nWARNING: Using mixup data augmentation technique!\\n\")\n",
    "    mixer = Mixup(\n",
    "        alpha=data_augmentation_config[\"techniques\"][\"mixup\"][\"alpha\"]\n",
    "    )\n",
    "\n",
    "if \"specmix\" in data_augmentation_config[\"techniques\"].keys():\n",
    "    print(\"\\nWARNING: Using specmix data augmentation technique!\\n\")\n",
    "    mixer = Specmix(\n",
    "        p=data_augmentation_config[\"p\"],\n",
    "        min_band_size=data_augmentation_config[\"techniques\"][\"specmix\"][\"min_band_size\"],\n",
    "        max_band_size=data_augmentation_config[\"techniques\"][\"specmix\"][\"max_band_size\"],\n",
    "        max_frequency_bands=data_augmentation_config[\"techniques\"][\"specmix\"][\"max_frequency_bands\"],\n",
    "        max_time_bands=data_augmentation_config[\"techniques\"][\"specmix\"][\"max_time_bands\"],\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "if \"cutmix\" in data_augmentation_config[\"techniques\"].keys():\n",
    "    print(\"\\nWARNING: Using cutmix data augmentation technique!\\n\")\n",
    "    mixer = Cutmix(\n",
    "        alpha=data_augmentation_config[\"techniques\"][\"cutmix\"][\"alpha\"],\n",
    "        p=data_augmentation_config[\"p\"]\n",
    "    )\n",
    "\n",
    "# creating the training dataloader\n",
    "training_dataloader = create_dataloader(\n",
    "    X=X_train,\n",
    "    y=y_train,\n",
    "    feature_config=feature_config,\n",
    "    wavelet_config=wavelet_config,\n",
    "    data_augmentation_config=data_augmentation_config,\n",
    "    num_workers=0,\n",
    "    mode=mode,\n",
    "    shuffle=False,\n",
    "    training=True,\n",
    "    batch_size=model_config[\"batch_size\"],\n",
    "    data_augment_target=data_augment_target,\n",
    "    worker_init_fn=seed_worker,\n",
    "    generator=g\n",
    ")\n",
    "\n",
    "# creating the validation dataloader\n",
    "validation_dataloader = create_dataloader(\n",
    "    X=X_valid,\n",
    "    y=y_valid,\n",
    "    feature_config=feature_config,\n",
    "    wavelet_config=wavelet_config,\n",
    "    data_augmentation_config=data_augmentation_config,\n",
    "    num_workers=0,\n",
    "    mode=mode,\n",
    "    shuffle=False,\n",
    "    training=False,\n",
    "    batch_size=model_config[\"batch_size\"],\n",
    "    data_augment_target=data_augment_target,\n",
    "    worker_init_fn=seed_worker,\n",
    "    generator=g\n",
    ")\n",
    "\n",
    "# creating the test dataloader\n",
    "test_dataloader = create_dataloader(\n",
    "    X=X_test,\n",
    "    y=y_test,\n",
    "    feature_config=feature_config,\n",
    "    wavelet_config=wavelet_config,\n",
    "    data_augmentation_config=None,\n",
    "    num_workers=0,\n",
    "    mode=params[\"mode\"],\n",
    "    shuffle=False,\n",
    "    training=False,\n",
    "    batch_size=params[\"model\"][\"batch_size\"],\n",
    "    data_augment_target=None,\n",
    "    worker_init_fn=seed_worker,\n",
    "    generator=g\n",
    ")\n",
    "\n",
    "# training loop\n",
    "for epoch in range(1, model_config[\"epochs\"] + 1):\n",
    "    print(f\"Epoch: {epoch}/{model_config['epochs']}\")\n",
    "\n",
    "    train_f1, train_loss = train(\n",
    "        device=device,\n",
    "        dataloader=training_dataloader,\n",
    "        optimizer=optimizer,\n",
    "        model=model,\n",
    "        loss=loss,\n",
    "        mixer=mixer\n",
    "    )\n",
    "\n",
    "    valid_f1, valid_loss = evaluate(\n",
    "        device=device,\n",
    "        dataloader=validation_dataloader,\n",
    "        model=model,\n",
    "        loss=loss\n",
    "    )\n",
    "\n",
    "    report = test(\n",
    "        model=model,\n",
    "        dataloader=test_dataloader,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    test_f1 = report[\"macro avg\"][\"f1-score\"]\n",
    "\n",
    "    print(f\"\\nEpoch: {epoch}\")\n",
    "    print(f\"Train F1-Score: {train_f1:1.6f}\")\n",
    "    print(f\"Train Loss: {train_loss:1.6f}\")\n",
    "    print(f\"Validation F1-Score: {valid_f1:1.6f}\")\n",
    "    print(f\"Validation Loss: {valid_loss:1.6f}\")\n",
    "    print(f\"Test F1-Score: {test_f1:1.6f}\\n\")\n",
    "\n",
    "    # updating learning rate\n",
    "    if not scheduler is None:\n",
    "        scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36100058",
   "metadata": {},
   "source": [
    "## CNN 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d0922b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config[\"name\"] = \"cnn2\"\n",
    "\n",
    "model = choose_model(\n",
    "    mode=mode,\n",
    "    model_name=model_config[\"name\"],\n",
    "    device=device\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    params=model.parameters(),\n",
    "    lr=model_config[\"learning_rate\"],\n",
    "    betas=(0.9, 0.98),\n",
    "    eps=1e-9,\n",
    "    weight_decay=0\n",
    ")\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "scheduler = None\n",
    "mixer = None\n",
    "\n",
    "# creating the model checkpoint object\n",
    "sbm = SaveBestModel(\n",
    "    output_dir=os.path.join(model_config[\"output_path\"], dataset, mode, model_config[\"name\"]),\n",
    "    model_name=model_config[\"name\"]\n",
    ")\n",
    "\n",
    "if model_config[\"use_lr_scheduler\"]:\n",
    "    print(\"\\nWARNING: Using learning rate scheduler!\\n\")\n",
    "    scheduler = StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "if \"mixup\" in data_augmentation_config[\"techniques\"].keys():\n",
    "    print(\"\\nWARNING: Using mixup data augmentation technique!\\n\")\n",
    "    mixer = Mixup(\n",
    "        alpha=data_augmentation_config[\"techniques\"][\"mixup\"][\"alpha\"]\n",
    "    )\n",
    "\n",
    "if \"specmix\" in data_augmentation_config[\"techniques\"].keys():\n",
    "    print(\"\\nWARNING: Using specmix data augmentation technique!\\n\")\n",
    "    mixer = Specmix(\n",
    "        p=data_augmentation_config[\"p\"],\n",
    "        min_band_size=data_augmentation_config[\"techniques\"][\"specmix\"][\"min_band_size\"],\n",
    "        max_band_size=data_augmentation_config[\"techniques\"][\"specmix\"][\"max_band_size\"],\n",
    "        max_frequency_bands=data_augmentation_config[\"techniques\"][\"specmix\"][\"max_frequency_bands\"],\n",
    "        max_time_bands=data_augmentation_config[\"techniques\"][\"specmix\"][\"max_time_bands\"],\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "if \"cutmix\" in data_augmentation_config[\"techniques\"].keys():\n",
    "    print(\"\\nWARNING: Using cutmix data augmentation technique!\\n\")\n",
    "    mixer = Cutmix(\n",
    "        alpha=data_augmentation_config[\"techniques\"][\"cutmix\"][\"alpha\"],\n",
    "        p=data_augmentation_config[\"p\"]\n",
    "    )\n",
    "\n",
    "# creating the training dataloader\n",
    "training_dataloader = create_dataloader(\n",
    "    X=X_train,\n",
    "    y=y_train,\n",
    "    feature_config=feature_config,\n",
    "    wavelet_config=wavelet_config,\n",
    "    data_augmentation_config=data_augmentation_config,\n",
    "    num_workers=0,\n",
    "    mode=mode,\n",
    "    shuffle=False,\n",
    "    training=True,\n",
    "    batch_size=model_config[\"batch_size\"],\n",
    "    data_augment_target=data_augment_target,\n",
    "    worker_init_fn=seed_worker,\n",
    "    generator=g\n",
    ")\n",
    "\n",
    "# creating the validation dataloader\n",
    "validation_dataloader = create_dataloader(\n",
    "    X=X_valid,\n",
    "    y=y_valid,\n",
    "    feature_config=feature_config,\n",
    "    wavelet_config=wavelet_config,\n",
    "    data_augmentation_config=data_augmentation_config,\n",
    "    num_workers=0,\n",
    "    mode=mode,\n",
    "    shuffle=False,\n",
    "    training=False,\n",
    "    batch_size=model_config[\"batch_size\"],\n",
    "    data_augment_target=data_augment_target,\n",
    "    worker_init_fn=seed_worker,\n",
    "    generator=g\n",
    ")\n",
    "\n",
    "# creating the test dataloader\n",
    "test_dataloader = create_dataloader(\n",
    "    X=X_test,\n",
    "    y=y_test,\n",
    "    feature_config=feature_config,\n",
    "    wavelet_config=wavelet_config,\n",
    "    data_augmentation_config=None,\n",
    "    num_workers=0,\n",
    "    mode=params[\"mode\"],\n",
    "    shuffle=False,\n",
    "    training=False,\n",
    "    batch_size=params[\"model\"][\"batch_size\"],\n",
    "    data_augment_target=None,\n",
    "    worker_init_fn=seed_worker,\n",
    "    generator=g\n",
    ")\n",
    "\n",
    "# training loop\n",
    "for epoch in range(1, model_config[\"epochs\"] + 1):\n",
    "    print(f\"Epoch: {epoch}/{model_config['epochs']}\")\n",
    "\n",
    "    train_f1, train_loss = train(\n",
    "        device=device,\n",
    "        dataloader=training_dataloader,\n",
    "        optimizer=optimizer,\n",
    "        model=model,\n",
    "        loss=loss,\n",
    "        mixer=mixer\n",
    "    )\n",
    "\n",
    "    valid_f1, valid_loss = evaluate(\n",
    "        device=device,\n",
    "        dataloader=validation_dataloader,\n",
    "        model=model,\n",
    "        loss=loss\n",
    "    )\n",
    "\n",
    "    report = test(\n",
    "        model=model,\n",
    "        dataloader=test_dataloader,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    test_f1 = report[\"macro avg\"][\"f1-score\"]\n",
    "\n",
    "    print(f\"\\nEpoch: {epoch}\")\n",
    "    print(f\"Train F1-Score: {train_f1:1.6f}\")\n",
    "    print(f\"Train Loss: {train_loss:1.6f}\")\n",
    "    print(f\"Validation F1-Score: {valid_f1:1.6f}\")\n",
    "    print(f\"Validation Loss: {valid_loss:1.6f}\")\n",
    "    print(f\"Test F1-Score: {test_f1:1.6f}\\n\")\n",
    "\n",
    "    # updating learning rate\n",
    "    if not scheduler is None:\n",
    "        scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94b9f5c",
   "metadata": {},
   "source": [
    "## CNN 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cee58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config[\"name\"] = \"cnn3\"\n",
    "\n",
    "# creating and defining the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available and model_config[\"use_gpu\"] else \"cpu\")\n",
    "\n",
    "model = choose_model(\n",
    "    mode=mode,\n",
    "    model_name=model_config[\"name\"],\n",
    "    device=device\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    params=model.parameters(),\n",
    "    lr=model_config[\"learning_rate\"],\n",
    "    betas=(0.9, 0.98),\n",
    "    eps=1e-9,\n",
    "    weight_decay=0\n",
    ")\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "scheduler = None\n",
    "mixer = None\n",
    "\n",
    "# creating the model checkpoint object\n",
    "sbm = SaveBestModel(\n",
    "    output_dir=os.path.join(model_config[\"output_path\"], dataset, mode, model_config[\"name\"]),\n",
    "    model_name=model_config[\"name\"]\n",
    ")\n",
    "\n",
    "if model_config[\"use_lr_scheduler\"]:\n",
    "    print(\"\\nWARNING: Using learning rate scheduler!\\n\")\n",
    "    scheduler = StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "if \"mixup\" in data_augmentation_config[\"techniques\"].keys():\n",
    "    print(\"\\nWARNING: Using mixup data augmentation technique!\\n\")\n",
    "    mixer = Mixup(\n",
    "        alpha=data_augmentation_config[\"techniques\"][\"mixup\"][\"alpha\"]\n",
    "    )\n",
    "\n",
    "if \"specmix\" in data_augmentation_config[\"techniques\"].keys():\n",
    "    print(\"\\nWARNING: Using specmix data augmentation technique!\\n\")\n",
    "    mixer = Specmix(\n",
    "        p=data_augmentation_config[\"p\"],\n",
    "        min_band_size=data_augmentation_config[\"techniques\"][\"specmix\"][\"min_band_size\"],\n",
    "        max_band_size=data_augmentation_config[\"techniques\"][\"specmix\"][\"max_band_size\"],\n",
    "        max_frequency_bands=data_augmentation_config[\"techniques\"][\"specmix\"][\"max_frequency_bands\"],\n",
    "        max_time_bands=data_augmentation_config[\"techniques\"][\"specmix\"][\"max_time_bands\"],\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "if \"cutmix\" in data_augmentation_config[\"techniques\"].keys():\n",
    "    print(\"\\nWARNING: Using cutmix data augmentation technique!\\n\")\n",
    "    mixer = Cutmix(\n",
    "        alpha=data_augmentation_config[\"techniques\"][\"cutmix\"][\"alpha\"],\n",
    "        p=data_augmentation_config[\"p\"]\n",
    "    )\n",
    "\n",
    "# creating the training dataloader\n",
    "training_dataloader = create_dataloader(\n",
    "    X=X_train,\n",
    "    y=y_train,\n",
    "    feature_config=feature_config,\n",
    "    wavelet_config=wavelet_config,\n",
    "    data_augmentation_config=data_augmentation_config,\n",
    "    num_workers=0,\n",
    "    mode=mode,\n",
    "    shuffle=False,\n",
    "    training=True,\n",
    "    batch_size=model_config[\"batch_size\"],\n",
    "    data_augment_target=data_augment_target,\n",
    "    worker_init_fn=seed_worker,\n",
    "    generator=g\n",
    ")\n",
    "\n",
    "# creating the validation dataloader\n",
    "validation_dataloader = create_dataloader(\n",
    "    X=X_valid,\n",
    "    y=y_valid,\n",
    "    feature_config=feature_config,\n",
    "    wavelet_config=wavelet_config,\n",
    "    data_augmentation_config=data_augmentation_config,\n",
    "    num_workers=0,\n",
    "    mode=mode,\n",
    "    shuffle=False,\n",
    "    training=False,\n",
    "    batch_size=model_config[\"batch_size\"],\n",
    "    data_augment_target=data_augment_target,\n",
    "    worker_init_fn=seed_worker,\n",
    "    generator=g\n",
    ")\n",
    "\n",
    "# creating the test dataloader\n",
    "test_dataloader = create_dataloader(\n",
    "    X=X_test,\n",
    "    y=y_test,\n",
    "    feature_config=feature_config,\n",
    "    wavelet_config=wavelet_config,\n",
    "    data_augmentation_config=None,\n",
    "    num_workers=0,\n",
    "    mode=params[\"mode\"],\n",
    "    shuffle=False,\n",
    "    training=False,\n",
    "    batch_size=params[\"model\"][\"batch_size\"],\n",
    "    data_augment_target=None,\n",
    "    worker_init_fn=seed_worker,\n",
    "    generator=g\n",
    ")\n",
    "\n",
    "# training loop\n",
    "for epoch in range(1, model_config[\"epochs\"] + 1):\n",
    "    print(f\"Epoch: {epoch}/{model_config['epochs']}\")\n",
    "\n",
    "    train_f1, train_loss = train(\n",
    "        device=device,\n",
    "        dataloader=training_dataloader,\n",
    "        optimizer=optimizer,\n",
    "        model=model,\n",
    "        loss=loss,\n",
    "        mixer=mixer\n",
    "    )\n",
    "\n",
    "    valid_f1, valid_loss = evaluate(\n",
    "        device=device,\n",
    "        dataloader=validation_dataloader,\n",
    "        model=model,\n",
    "        loss=loss\n",
    "    )\n",
    "\n",
    "    report = test(\n",
    "        model=model,\n",
    "        dataloader=test_dataloader,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    test_f1 = report[\"macro avg\"][\"f1-score\"]\n",
    "\n",
    "    print(f\"\\nEpoch: {epoch}\")\n",
    "    print(f\"Train F1-Score: {train_f1:1.6f}\")\n",
    "    print(f\"Train Loss: {train_loss:1.6f}\")\n",
    "    print(f\"Validation F1-Score: {valid_f1:1.6f}\")\n",
    "    print(f\"Validation Loss: {valid_loss:1.6f}\")\n",
    "    print(f\"Test F1-Score: {test_f1:1.6f}\\n\")\n",
    "\n",
    "    # updating learning rate\n",
    "    if not scheduler is None:\n",
    "        scheduler.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
