{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb410d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "pip install PyWavelets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d4d1abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'pytorch_wavelets'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /media/greca/HD/GitHub/ser-pytorch/notebooks/pytorch_wavelets\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: numpy in /media/greca/HD/anaconda3/envs/ser/lib/python3.8/site-packages (from pytorch-wavelets==1.3.0) (1.23.5)\n",
      "Requirement already satisfied: six in /media/greca/HD/anaconda3/envs/ser/lib/python3.8/site-packages (from pytorch-wavelets==1.3.0) (1.16.0)\n",
      "Requirement already satisfied: torch in /media/greca/HD/anaconda3/envs/ser/lib/python3.8/site-packages (from pytorch-wavelets==1.3.0) (1.13.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /media/greca/HD/anaconda3/envs/ser/lib/python3.8/site-packages (from torch->pytorch-wavelets==1.3.0) (11.7.99)\n",
      "Requirement already satisfied: typing-extensions in /media/greca/HD/anaconda3/envs/ser/lib/python3.8/site-packages (from torch->pytorch-wavelets==1.3.0) (4.5.0)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /media/greca/HD/anaconda3/envs/ser/lib/python3.8/site-packages (from torch->pytorch-wavelets==1.3.0) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /media/greca/HD/anaconda3/envs/ser/lib/python3.8/site-packages (from torch->pytorch-wavelets==1.3.0) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /media/greca/HD/anaconda3/envs/ser/lib/python3.8/site-packages (from torch->pytorch-wavelets==1.3.0) (8.5.0.96)\n",
      "Requirement already satisfied: wheel in /media/greca/HD/anaconda3/envs/ser/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch->pytorch-wavelets==1.3.0) (0.38.4)\n",
      "Requirement already satisfied: setuptools in /media/greca/HD/anaconda3/envs/ser/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch->pytorch-wavelets==1.3.0) (66.0.0)\n",
      "Building wheels for collected packages: pytorch-wavelets\n",
      "  Building wheel for pytorch-wavelets (setup.py): started\n",
      "  Building wheel for pytorch-wavelets (setup.py): finished with status 'done'\n",
      "  Created wheel for pytorch-wavelets: filename=pytorch_wavelets-1.3.0-py3-none-any.whl size=54851 sha256=8eb842a3b29fe5fa00de6912abdcbdd69dc585d76c3b885de71d39e609aa66e3\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-4287ticv/wheels/d9/6f/3c/68cbd8ac7dc59a9d948be15c30f0c07cbf4b8432b16e42b125\n",
      "Successfully built pytorch-wavelets\n",
      "Installing collected packages: pytorch-wavelets\n",
      "  Attempting uninstall: pytorch-wavelets\n",
      "    Found existing installation: pytorch-wavelets 1.3.0\n",
      "    Uninstalling pytorch-wavelets-1.3.0:\n",
      "      Successfully uninstalled pytorch-wavelets-1.3.0\n",
      "Successfully installed pytorch-wavelets-1.3.0\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "git clone https://github.com/fbcotter/pytorch_wavelets.git\n",
    "cd pytorch_wavelets\n",
    "pip install .\n",
    "cd ..\n",
    "rm -r pytorch_wavelets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce0e52af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pywt\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import pywt\n",
    "import random\n",
    "from audiomentations import Normalize\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pytorch_wavelets import DWTForward, DTCWTForward, ScatLayer\n",
    "from typing import Tuple, List, Dict\n",
    "\n",
    "# Making sure the experiments are reproducible\n",
    "seed = 2109\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "eff67dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_propor_train_dataframe(\n",
    "    path: str\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Creates a PROPOR 2022's pandas DataFrame containing\n",
    "    all the training files using the same structure as the\n",
    "    `test_ser_metadata.csv` file.\n",
    "    \n",
    "    Args:\n",
    "        path (str): the path to the CSV file.\n",
    "    \n",
    "    Returns:\n",
    "        df (pd.DataFrame): the pandas DataFrame.\n",
    "    \"\"\"\n",
    "    wav_files = [\n",
    "        file\n",
    "        for file in os.listdir(path)\n",
    "        if file.endswith(\".wav\")\n",
    "    ]\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    for wav in wav_files:\n",
    "        wav_file = os.path.basename(wav)\n",
    "        wav_file = wav_file.split(\"/\")[0]\n",
    "        label = wav_file.split(\"_\")[-1].replace(\".wav\", \"\")\n",
    "        \n",
    "        row = pd.DataFrame({\n",
    "            \"file\": [os.path.join(path, wav_file)],\n",
    "            \"label\": [label],\n",
    "            \"wav_file\": [wav_file]\n",
    "        })\n",
    "        \n",
    "        df = pd.concat(\n",
    "            [df, row],\n",
    "            axis=0\n",
    "        )\n",
    "    \n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "def stereo_to_mono(\n",
    "    audio: torch.Tensor\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Converts a stereo audio to mono.\n",
    "    \n",
    "    Args:\n",
    "        audio (torch.Tensor): the audio's waveform (stereo).\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: the audio's waveform (mono).\n",
    "    \"\"\"\n",
    "    audio = torch.mean(audio, dim=0, keepdim=True)\n",
    "    return audio\n",
    "\n",
    "def read_audio(\n",
    "    path: str,\n",
    "    to_mono: bool = True,\n",
    "    sample_rate: int = None\n",
    ") -> Tuple[torch.Tensor, int]:\n",
    "    \"\"\"\n",
    "    Reads a audio file.\n",
    "\n",
    "    Args:\n",
    "        path (str): the audio file's path.\n",
    "        to_mono (bool, optional): convert the signal to mono. Defaults to True.\n",
    "        sample_rate (int, optional): resample the audio to that specific sample rate\n",
    "                                     (None if you won't resample). Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[torch.Tensor, int]: the audio waveform and the sample rate.\n",
    "    \"\"\"\n",
    "    audio, sr = torchaudio.load(filepath=path)\n",
    "    \n",
    "    if sample_rate is not None and sample_rate != sr:\n",
    "        audio = resample_audio(\n",
    "            audio=audio,\n",
    "            sample_rate=sr,\n",
    "            new_sample_rate=sample_rate\n",
    "        )\n",
    "    \n",
    "    if to_mono and audio.shape[0] > 1:\n",
    "        audio = stereo_to_mono(audio=audio)\n",
    "    \n",
    "    return audio, sr\n",
    "\n",
    "def resample_audio(\n",
    "    audio: torch.Tensor,\n",
    "    sample_rate: int,\n",
    "    new_sample_rate: int\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Resamples a given audio.\n",
    "\n",
    "    Args:\n",
    "        audio (torch.Tensor): the audio's waveform.\n",
    "        sample_rate (int): the original audio's sample rate.\n",
    "        new_sample_rate (int): the new audio's sample rate.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: the resampled audio's waveform.\n",
    "    \"\"\"\n",
    "    transform = torchaudio.transforms.Resample(\n",
    "        orig_freq=sample_rate,\n",
    "        new_freq=new_sample_rate\n",
    "    )\n",
    "    audio = transform(audio)\n",
    "    return audio\n",
    "\n",
    "def extract_spectrogram(\n",
    "    audio: torch.Tensor,\n",
    "    n_fft: int,\n",
    "    hop_length: int\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Extracts the spectrogram of a given audio.\n",
    "    \n",
    "    Args:\n",
    "        audio (torch.Tensor): the audio's waveform.\n",
    "        n_fft (int): the number of fft.\n",
    "        hop_length (int): the hop length.\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: the extracted Spectogram.\n",
    "    \"\"\"\n",
    "    transform = torchaudio.transforms.Spectrogram(\n",
    "        n_fft=n_fft,\n",
    "        hop_length=hop_length\n",
    "    )\n",
    "    spectrogram = transform(audio)\n",
    "    return spectrogram\n",
    "\n",
    "def extract_melspectrogram(\n",
    "    audio: torch.Tensor,\n",
    "    sample_rate: int,\n",
    "    n_fft: int,\n",
    "    hop_length: int,\n",
    "    n_mels: int\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Extracts the mel spectrogram of a given audio.\n",
    "    \n",
    "    Args:\n",
    "        audio (np.ndarray): the audio's waveform.\n",
    "        sample_rate (int): the audio's sample rate.\n",
    "        n_fft (int): the number of fft.\n",
    "        hop_length (int): the hop length.\n",
    "        n_mels (int): the number of mels.\n",
    "        f_min (int): the minimum frequency.\n",
    "        f_max (int): the maximum frequency.\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: the extracted Mel Spectrogram.\n",
    "    \"\"\"\n",
    "    transform = torchaudio.transforms.MelSpectrogram(\n",
    "        sample_rate=sample_rate,\n",
    "        n_fft=n_fft,\n",
    "        hop_length=hop_length,\n",
    "        n_mels=n_mels,\n",
    "        power=1\n",
    "    )\n",
    "    mel_spectrogram = transform(audio)\n",
    "    return mel_spectrogram\n",
    "\n",
    "def pad_features(\n",
    "    features: List,\n",
    "    max_height: int,\n",
    "    max_width: int\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Auxiliary function to pad the features.\n",
    "    \n",
    "    Args:\n",
    "        features (List): the features that will be padded (mfcc, spectogram or mel_spectogram).\n",
    "        max_height (int): the height max value.\n",
    "        max_width (int): the width max value.\n",
    "    \n",
    "    Returns:\n",
    "        List: the padded features.\n",
    "    \"\"\"\n",
    "    features = [\n",
    "        F.pad(f, (0, max_width - f.size(2), 0, max_height - f.size(1)))\n",
    "        for f in features\n",
    "    ]\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f12bd97a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>label</th>\n",
       "      <th>wav_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/media/greca/HD/Datasets/PROPOR 2022/data_trai...</td>\n",
       "      <td>1</td>\n",
       "      <td>bpubdl02_segment247_non-neutral-male.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/media/greca/HD/Datasets/PROPOR 2022/data_trai...</td>\n",
       "      <td>0</td>\n",
       "      <td>bpubmn14_segment89_neutral.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/media/greca/HD/Datasets/PROPOR 2022/data_trai...</td>\n",
       "      <td>0</td>\n",
       "      <td>bfamdl26_segment93_neutral.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/media/greca/HD/Datasets/PROPOR 2022/data_trai...</td>\n",
       "      <td>0</td>\n",
       "      <td>bfammn27_segment275_neutral.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/media/greca/HD/Datasets/PROPOR 2022/data_trai...</td>\n",
       "      <td>0</td>\n",
       "      <td>bfamcv02_segment276_neutral.wav</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                file  label  \\\n",
       "0  /media/greca/HD/Datasets/PROPOR 2022/data_trai...      1   \n",
       "1  /media/greca/HD/Datasets/PROPOR 2022/data_trai...      0   \n",
       "2  /media/greca/HD/Datasets/PROPOR 2022/data_trai...      0   \n",
       "3  /media/greca/HD/Datasets/PROPOR 2022/data_trai...      0   \n",
       "4  /media/greca/HD/Datasets/PROPOR 2022/data_trai...      0   \n",
       "\n",
       "                                   wav_file  \n",
       "0  bpubdl02_segment247_non-neutral-male.wav  \n",
       "1            bpubmn14_segment89_neutral.wav  \n",
       "2            bfamdl26_segment93_neutral.wav  \n",
       "3           bfammn27_segment275_neutral.wav  \n",
       "4           bfamcv02_segment276_neutral.wav  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "propor_path = \"/media/greca/HD/Datasets/PROPOR 2022/data_train/train\"\n",
    "df = create_propor_train_dataframe(propor_path)\n",
    "df[\"label\"] = df[\"label\"].replace({\n",
    "    \"neutral\": 0,\n",
    "    \"non-neutral-male\": 1,\n",
    "    \"non-neutral-female\": 2\n",
    "})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "591a62e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "data = []\n",
    "audios = []\n",
    "\n",
    "for file, label in zip(df[\"file\"], df[\"label\"]):\n",
    "    audio, sr = read_audio(\n",
    "        path=file,\n",
    "        to_mono=True,\n",
    "        sample_rate=None\n",
    "    )\n",
    "    \n",
    "    feat = extract_melspectrogram(\n",
    "        audio=audio,\n",
    "        sample_rate=sr,\n",
    "        n_fft=1024,\n",
    "        hop_length=512,\n",
    "        n_mels=60\n",
    "    )\n",
    "    \n",
    "#     feat = extract_spectrogram(\n",
    "#         audio=audio,\n",
    "#         n_fft=1024,\n",
    "#         hop_length=512    \n",
    "#     )\n",
    "    \n",
    "    data.append(feat)\n",
    "    labels.append(label)\n",
    "    audios.append(audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43c9785d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([625, 1, 60, 439]) torch.Size([625])\n"
     ]
    }
   ],
   "source": [
    "max_height = max([x.size(1) for x in data])\n",
    "max_width = max([x.size(2) for x in data])\n",
    "\n",
    "data = pad_features(\n",
    "    features=data,\n",
    "    max_height=max_height,\n",
    "    max_width=max_width\n",
    ")\n",
    "\n",
    "data = torch.cat(data, 0).to(dtype=torch.float32)\n",
    "data = data.unsqueeze(1)\n",
    "labels = torch.as_tensor(labels).to(dtype=torch.long)\n",
    "\n",
    "print(data.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bcf079f",
   "metadata": {},
   "source": [
    "## Comparing PyTorch Wavelets vs PyWavelets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9cd123fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 14, 37])\n",
      "4\n",
      "torch.Size([1, 1, 3, 35, 225])\n",
      "torch.Size([1, 1, 3, 23, 118])\n",
      "torch.Size([1, 1, 3, 17, 64])\n",
      "torch.Size([1, 1, 3, 14, 37])\n",
      "\n",
      "\n",
      "(1, 1, 14, 37)\n",
      "(1, 1, 35, 225)\n",
      "(1, 1, 35, 225)\n",
      "(1, 1, 35, 225)\n",
      "(1, 1, 23, 118)\n",
      "(1, 1, 23, 118)\n",
      "(1, 1, 23, 118)\n",
      "(1, 1, 17, 64)\n",
      "(1, 1, 17, 64)\n",
      "(1, 1, 17, 64)\n",
      "(1, 1, 14, 37)\n",
      "(1, 1, 14, 37)\n",
      "(1, 1, 14, 37)\n",
      "\n",
      "(1, 1, 103, 481)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/greca/HD/anaconda3/envs/ser/lib/python3.8/site-packages/pywt/_multilevel.py:43: UserWarning: Level value of 4 is too high: all coefficients will experience boundary effects.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "audio_test = data[0, :, :, :].unsqueeze(0)\n",
    "maxlevel = 4\n",
    "wavelet=\"db6\"\n",
    "\n",
    "# PyTorch Wavelets\n",
    "xfm = DWTForward(\n",
    "    J=maxlevel,\n",
    "    wave=wavelet, \n",
    "    mode=\"symmetric\"\n",
    ")\n",
    "Yl, Yh = xfm(audio_test)\n",
    "print(Yl.shape)\n",
    "print(len(Yh))\n",
    "\n",
    "for i in range(len(Yh)):\n",
    "    print(Yh[i].shape)\n",
    "\n",
    "print(); print();\n",
    "\n",
    "# PyWavelets\n",
    "coeffs = pywt.wavedec2(\n",
    "    data=audio_test,\n",
    "    level=maxlevel,\n",
    "    wavelet=wavelet, \n",
    "    mode=\"symmetric\",\n",
    "    axes=(-2,-1)\n",
    ")\n",
    "\n",
    "print(coeffs[0].shape)\n",
    "for j in range(maxlevel):\n",
    "    for b in range(3):\n",
    "        print(coeffs[maxlevel-j][b].shape)\n",
    "\n",
    "print()\n",
    "\n",
    "arr, coeff_slices = pywt.coeffs_to_array(coeffs, axes=(-2, -1))\n",
    "print(arr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb5d6db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code taken from: https://github.com/fbcotter/pytorch_wavelets/blob/master/tests/test_dwt.py\n",
    "from pytorch_wavelets import DWTForward, DWTInverse\n",
    "import numpy as np\n",
    "\n",
    "PREC_FLT = 3\n",
    "PREC_DBL = 7\n",
    "J = 4\n",
    "wave = \"db6\"\n",
    "mode = \"symmetric\"\n",
    "x = audio_test\n",
    "\n",
    "dwt = DWTForward(J=J, wave=wave, mode=mode)\n",
    "yl, yh = dwt(x)\n",
    "\n",
    "coeffs = pywt.wavedec2(x.cpu().numpy(), wave, level=J, axes=(-2,-1),\n",
    "                       mode=mode)\n",
    "np.testing.assert_array_almost_equal(yl.cpu(), coeffs[0], decimal=PREC_FLT)\n",
    "for j in range(J):\n",
    "    for b in range(3):\n",
    "        np.testing.assert_array_almost_equal(\n",
    "            coeffs[J-j][b], yh[j][:,:,b].cpu(), decimal=PREC_FLT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "839f1637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 103, 481)\n"
     ]
    }
   ],
   "source": [
    "arr, coeff_slices = pywt.coeffs_to_array(coeffs, axes=(-2, -1))\n",
    "print(arr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dec1745a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 103, 481)\n"
     ]
    }
   ],
   "source": [
    "arr, coeff_slices = pywt.coeffs_to_array(coeffs, axes=(-2, -1))\n",
    "print(arr.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82e4611",
   "metadata": {},
   "source": [
    "## PyTorch Wavelets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79842c3",
   "metadata": {},
   "source": [
    "### Extracting the Wavelets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e05e077",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SER_Dataset_PTH(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        X: torch.Tensor,\n",
    "        y: torch.Tensor\n",
    "    ) -> None:\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(\n",
    "        self,\n",
    "        index: int\n",
    "    ) -> Dict:\n",
    "        batch = {}\n",
    "        batch[\"features\"] = self.X[index, :, :, :]\n",
    "        batch[\"labels\"] = self.y[index]\n",
    "        return batch\n",
    "    \n",
    "def create_dataloader_pth(\n",
    "    X: torch.Tensor,\n",
    "    y: torch.Tensor\n",
    ") -> DataLoader:\n",
    "    dataset = SER_Dataset_PTH(X, y)\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=32,\n",
    "        num_workers=0,\n",
    "        drop_last=True\n",
    "    )\n",
    "    \n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2db695eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([625, 1, 17, 64])\n",
      "3\n",
      "torch.Size([625, 1, 3, 35, 225])\n",
      "torch.Size([625, 1, 3, 23, 118])\n",
      "torch.Size([625, 1, 3, 17, 64])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dwt = DWTForward(wave=\"db6\", J=3, mode=\"symmetric\")\n",
    "yl, yh = dwt(data)\n",
    "\n",
    "print(yl.shape)\n",
    "print(len(yh))\n",
    "print(yh[0].shape)\n",
    "print(yh[1].shape)\n",
    "print(yh[2].shape)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "819d60ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([625, 1, 16, 110])\n",
      "3\n",
      "torch.Size([625, 1, 6, 30, 220, 2])\n",
      "torch.Size([625, 1, 6, 15, 110, 2])\n",
      "torch.Size([625, 1, 6, 8, 55, 2])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dtcwt = DTCWTForward(J=3, biort=\"near_sym_b\", qshift=\"qshift_b\")\n",
    "yl, yh = dtcwt(data)\n",
    "\n",
    "print(yl.shape)\n",
    "print(len(yh))\n",
    "print(yh[0].shape)\n",
    "print(yh[1].shape)\n",
    "print(yh[2].shape)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "475414d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([625, 7, 30, 220])\n"
     ]
    }
   ],
   "source": [
    "scat = ScatLayer()\n",
    "output = scat(data)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533fadcc",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "49284c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselinePTH(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            ScatLayer(),\n",
    "            nn.Conv2d(\n",
    "                in_channels=7,\n",
    "                out_channels=64,\n",
    "                kernel_size=(2, 2),\n",
    "                padding=\"valid\"\n",
    "            ),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(\n",
    "                kernel_size=(2, 2)\n",
    "            ),\n",
    "            nn.Conv2d(\n",
    "                in_channels=64,\n",
    "                out_channels=128,\n",
    "                kernel_size=(2, 2),\n",
    "                padding=\"valid\"\n",
    "            ),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(\n",
    "                kernel_size=(2, 2)\n",
    "            ),\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(\n",
    "                in_features=41472,\n",
    "                out_features=3\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    def forward(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        return self.model(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6e18e1d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training epoch: 1/30\n",
      "Training loss: 2.101768\n",
      "Training F1: 0.310213\n",
      "\n",
      "Training epoch: 2/30\n",
      "Training loss: 0.978775\n",
      "Training F1: 0.444106\n",
      "\n",
      "Training epoch: 3/30\n",
      "Training loss: 0.926789\n",
      "Training F1: 0.460981\n",
      "\n",
      "Training epoch: 4/30\n",
      "Training loss: 1.142287\n",
      "Training F1: 0.486497\n",
      "\n",
      "Training epoch: 5/30\n",
      "Training loss: 0.713688\n",
      "Training F1: 0.582194\n",
      "\n",
      "Training epoch: 6/30\n",
      "Training loss: 0.709239\n",
      "Training F1: 0.607228\n",
      "\n",
      "Training epoch: 7/30\n",
      "Training loss: 0.715909\n",
      "Training F1: 0.590082\n",
      "\n",
      "Training epoch: 8/30\n",
      "Training loss: 0.638277\n",
      "Training F1: 0.607354\n",
      "\n",
      "Training epoch: 9/30\n",
      "Training loss: 0.445954\n",
      "Training F1: 0.684647\n",
      "\n",
      "Training epoch: 10/30\n",
      "Training loss: 0.361411\n",
      "Training F1: 0.735316\n",
      "\n",
      "Training epoch: 11/30\n",
      "Training loss: 0.330701\n",
      "Training F1: 0.754670\n",
      "\n",
      "Training epoch: 12/30\n",
      "Training loss: 0.253081\n",
      "Training F1: 0.807139\n",
      "\n",
      "Training epoch: 13/30\n",
      "Training loss: 0.286743\n",
      "Training F1: 0.769616\n",
      "\n",
      "Training epoch: 14/30\n",
      "Training loss: 0.232459\n",
      "Training F1: 0.848193\n",
      "\n",
      "Training epoch: 15/30\n",
      "Training loss: 0.283899\n",
      "Training F1: 0.778180\n",
      "\n",
      "Training epoch: 16/30\n",
      "Training loss: 0.254762\n",
      "Training F1: 0.774587\n",
      "\n",
      "Training epoch: 17/30\n",
      "Training loss: 0.336696\n",
      "Training F1: 0.803928\n",
      "\n",
      "Training epoch: 18/30\n",
      "Training loss: 0.310383\n",
      "Training F1: 0.781122\n",
      "\n",
      "Training epoch: 19/30\n",
      "Training loss: 0.186673\n",
      "Training F1: 0.843156\n",
      "\n",
      "Training epoch: 20/30\n",
      "Training loss: 0.212515\n",
      "Training F1: 0.858513\n",
      "\n",
      "Training epoch: 21/30\n",
      "Training loss: 0.155706\n",
      "Training F1: 0.876662\n",
      "\n",
      "Training epoch: 22/30\n",
      "Training loss: 0.202085\n",
      "Training F1: 0.832479\n",
      "\n",
      "Training epoch: 23/30\n",
      "Training loss: 0.197166\n",
      "Training F1: 0.833806\n",
      "\n",
      "Training epoch: 24/30\n",
      "Training loss: 0.166189\n",
      "Training F1: 0.856019\n",
      "\n",
      "Training epoch: 25/30\n",
      "Training loss: 0.162491\n",
      "Training F1: 0.873797\n",
      "\n",
      "Training epoch: 26/30\n",
      "Training loss: 0.193930\n",
      "Training F1: 0.877918\n",
      "\n",
      "Training epoch: 27/30\n",
      "Training loss: 0.139051\n",
      "Training F1: 0.898161\n",
      "\n",
      "Training epoch: 28/30\n",
      "Training loss: 0.143435\n",
      "Training F1: 0.878315\n",
      "\n",
      "Training epoch: 29/30\n",
      "Training loss: 0.156038\n",
      "Training F1: 0.885087\n",
      "\n",
      "Training epoch: 30/30\n",
      "Training loss: 0.153110\n",
      "Training F1: 0.897120\n"
     ]
    }
   ],
   "source": [
    "dataloader = create_dataloader_pth(data, labels)\n",
    "device = torch.device(\"cpu\")\n",
    "model = BaselinePTH().to(device)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.001)\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "epochs = 30\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    print(f\"\\nTraining epoch: {epoch}/{epochs}\")\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_f1 = 0.0\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        data = batch[\"features\"].to(device)\n",
    "        target = batch[\"labels\"].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(data)\n",
    "        \n",
    "        l = loss(output, target)\n",
    "        train_loss += l.item()\n",
    "        \n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        prediction = output.argmax(dim=-1, keepdim=True)\n",
    "        train_f1 += f1_score(\n",
    "            target.detach().cpu().numpy(),\n",
    "            prediction.detach().cpu().numpy(),\n",
    "            average=\"macro\"\n",
    "        )\n",
    "        \n",
    "    train_loss /= len(dataloader)\n",
    "    train_f1 /= len(dataloader)\n",
    "    \n",
    "    print(f\"Training loss: {train_loss:1.6f}\")\n",
    "    print(f\"Training F1: {train_f1:1.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292971b6",
   "metadata": {},
   "source": [
    "## PyWavelets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ee97a3",
   "metadata": {},
   "source": [
    "### Extracting the Wavelets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6c39b976",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stereo_to_mono(\n",
    "    audio: torch.Tensor\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Converts a stereo audio to mono.\n",
    "    \n",
    "    Args:\n",
    "        audio (torch.Tensor): the audio's waveform (stereo).\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: the audio's waveform (mono).\n",
    "    \"\"\"\n",
    "    audio = torch.mean(audio, dim=0, keepdim=True)\n",
    "    return audio\n",
    "\n",
    "def read_audio(\n",
    "    path: str,\n",
    "    to_mono: bool = True,\n",
    "    sample_rate: int = None\n",
    ") -> Tuple[torch.Tensor, int]:\n",
    "    \"\"\"\n",
    "    Reads a audio file.\n",
    "\n",
    "    Args:\n",
    "        path (str): the audio file's path.\n",
    "        to_mono (bool, optional): convert the signal to mono. Defaults to True.\n",
    "        sample_rate (int, optional): resample the audio to that specific sample rate\n",
    "                                     (None if you won't resample). Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[torch.Tensor, int]: the audio waveform and the sample rate.\n",
    "    \"\"\"\n",
    "    audio, sr = torchaudio.load(filepath=path)\n",
    "    \n",
    "    if sample_rate is not None and sample_rate != sr:\n",
    "        audio = resample_audio(\n",
    "            audio=audio,\n",
    "            sample_rate=sr,\n",
    "            new_sample_rate=sample_rate\n",
    "        )\n",
    "        sr = sample_rate\n",
    "    \n",
    "    if to_mono and audio.shape[0] > 1:\n",
    "        audio = stereo_to_mono(audio=audio)\n",
    "    \n",
    "    return audio, sr\n",
    "\n",
    "def torch_to_numpy(\n",
    "    tensor: torch.Tensor\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Converts a torch's tensor to numpy.\n",
    "\n",
    "    Args:\n",
    "        tensor (torch.Tensor): torch's tensor.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: numpy's tensor.\n",
    "    \"\"\"\n",
    "    np_tensor = tensor.detach().permute(1, 0).numpy()\n",
    "    np_tensor = np_tensor.astype(np.float32)\n",
    "    return np_tensor\n",
    "\n",
    "def numpy_to_torch(\n",
    "    tensor: np.ndarray\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Converts a numpy's tensor to torch.\n",
    "\n",
    "    Args:\n",
    "        tensor (np.ndarray): numpy's tensor.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: torch's tensor.\n",
    "    \"\"\"\n",
    "    torch_tensor = torch.from_numpy(tensor).permute(1, 0)\n",
    "    torch_tensor = torch_tensor.to(dtype=torch.float32)\n",
    "    return torch_tensor\n",
    "\n",
    "def create_propor_train_dataframe(\n",
    "    path: str\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Creates a PROPOR 2022's pandas DataFrame containing\n",
    "    all the training files using the same structure as the\n",
    "    `test_ser_metadata.csv` file.\n",
    "    \n",
    "    Args:\n",
    "        path (str): the path to the CSV file.\n",
    "    \n",
    "    Returns:\n",
    "        df (pd.DataFrame): the pandas DataFrame.\n",
    "    \"\"\"\n",
    "    wav_files = [\n",
    "        file\n",
    "        for file in os.listdir(path)\n",
    "        if file.endswith(\".wav\")\n",
    "    ]\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    for wav in wav_files:\n",
    "        wav_file = os.path.basename(wav)\n",
    "        wav_file = wav_file.split(\"/\")[0]\n",
    "        label = wav_file.split(\"_\")[-1].replace(\".wav\", \"\")\n",
    "        \n",
    "        row = pd.DataFrame({\n",
    "            \"file\": [wav_file],\n",
    "            \"label\": [label],\n",
    "            \"wav_file\": [wav_file]\n",
    "        })\n",
    "        \n",
    "        df = pd.concat(\n",
    "            [df, row],\n",
    "            axis=0\n",
    "        )\n",
    "    \n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "def wavelet_from_raw_audio(\n",
    "    df: pd.DataFrame,\n",
    "    wavelet: str,\n",
    "    maxlevel: int,\n",
    "    type: str,\n",
    "    params: dict\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extract the wavelet from the raw audio.\n",
    "\n",
    "    Args:\n",
    "        spectrogram (torch.Tensor): the raw audios.\n",
    "        labels (torch.Tensor): the raw audios' labels.\n",
    "        wavelet (str): the wavelet's name.\n",
    "        maxlevel (int): the wavelet's max level.\n",
    "        type (str): which wavelet to extract (packet).\n",
    "        params (Dict): the parameters dict.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: the extracted wavelet packet.\n",
    "    \"\"\"\n",
    "    wavelet_df = pd.DataFrame()\n",
    "    normalizer = Normalize(p=1)\n",
    "    \n",
    "    if type == \"packet\":\n",
    "        for label, file_path in zip(df[\"label\"], df[\"wav_file\"]):\n",
    "            mean_nodes = []\n",
    "            audio, sr = read_audio(\n",
    "                path=file_path,\n",
    "                to_mono=params[\"feature\"][\"to_mono\"],\n",
    "                sample_rate=params[\"feature\"][\"sample_rate\"]\n",
    "            )\n",
    "            audio = audio.squeeze().numpy()\n",
    "            audio = normalizer(audio, params[\"feature\"][\"sample_rate\"])\n",
    "            audio = np.squeeze(audio)\n",
    "            \n",
    "            wp = pywt.WaveletPacket(\n",
    "                data=audio,\n",
    "                wavelet=wavelet,\n",
    "                mode=\"symmetric\",\n",
    "                maxlevel=maxlevel\n",
    "            )\n",
    "            \n",
    "            if wp.maxlevel > 0:\n",
    "                nodes = [node.path for node in wp.get_level(maxlevel, \"natural\")]\n",
    "                \n",
    "                for node in nodes:\n",
    "                    data = wp[node].data\n",
    "                    data = np.multiply(data, data)\n",
    "                    data = sum(data)/len(data)\n",
    "                    # data = sum(data)\n",
    "                    mean_data = torch.as_tensor(data)\n",
    "                    mean_data = mean_data.to(torch.float32).item()\n",
    "                    mean_nodes.append([mean_data])\n",
    "                \n",
    "                row = pd.DataFrame(mean_nodes).T\n",
    "                row.columns = nodes\n",
    "                row[\"target\"] = label\n",
    "                wavelet_df = pd.concat([wavelet_df, row], axis=0)\n",
    "                \n",
    "    wavelet_df = wavelet_df.reset_index(drop=True)\n",
    "    return wavelet_df\n",
    "\n",
    "propor_train_path = \"/media/greca/HD/Datasets/PROPOR 2022/data_train/train\"\n",
    "propor_train_df = create_propor_train_dataframe(propor_train_path)\n",
    "propor_train_df[\"wav_file\"] = propor_train_df[\"wav_file\"].apply(lambda x: os.path.join(propor_train_path, x))\n",
    "\n",
    "wavelet_df = wavelet_from_raw_audio(\n",
    "    df=propor_train_df,\n",
    "    wavelet=\"db6\",\n",
    "    maxlevel=3,\n",
    "    type=\"packet\",\n",
    "    params={\n",
    "        \"feature\":{\n",
    "            \"to_mono\": True,\n",
    "            \"sample_rate\": 16000\n",
    "        }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3d9c3ed8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaa</th>\n",
       "      <th>aad</th>\n",
       "      <th>ada</th>\n",
       "      <th>add</th>\n",
       "      <th>daa</th>\n",
       "      <th>dad</th>\n",
       "      <th>dda</th>\n",
       "      <th>ddd</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.254451</td>\n",
       "      <td>0.019808</td>\n",
       "      <td>0.007313</td>\n",
       "      <td>0.008119</td>\n",
       "      <td>0.000257</td>\n",
       "      <td>0.000768</td>\n",
       "      <td>0.006321</td>\n",
       "      <td>0.001077</td>\n",
       "      <td>non-neutral-male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.078333</td>\n",
       "      <td>0.001070</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.088213</td>\n",
       "      <td>0.043541</td>\n",
       "      <td>0.000455</td>\n",
       "      <td>0.008713</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000135</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.063264</td>\n",
       "      <td>0.004273</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>0.000697</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.055455</td>\n",
       "      <td>0.038655</td>\n",
       "      <td>0.006121</td>\n",
       "      <td>0.014573</td>\n",
       "      <td>0.000165</td>\n",
       "      <td>0.000527</td>\n",
       "      <td>0.004849</td>\n",
       "      <td>0.000741</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        aaa       aad       ada       add       daa       dad       dda  \\\n",
       "0  0.254451  0.019808  0.007313  0.008119  0.000257  0.000768  0.006321   \n",
       "1  0.078333  0.001070  0.000028  0.000110  0.000010  0.000011  0.000020   \n",
       "2  0.088213  0.043541  0.000455  0.008713  0.000005  0.000032  0.000135   \n",
       "3  0.063264  0.004273  0.000110  0.000697  0.000008  0.000016  0.000062   \n",
       "4  0.055455  0.038655  0.006121  0.014573  0.000165  0.000527  0.004849   \n",
       "\n",
       "        ddd            target  \n",
       "0  0.001077  non-neutral-male  \n",
       "1  0.000008           neutral  \n",
       "2  0.000096           neutral  \n",
       "3  0.000022           neutral  \n",
       "4  0.000741           neutral  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wavelet_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1762bd26",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e4d1065d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SER_Dataset_PYWT(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        df: pd.DataFrame\n",
    "    ) -> None:\n",
    "        self.X = df.drop(columns=[\"target\"])\n",
    "        self.y = df[\"target\"]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(\n",
    "        self,\n",
    "        index: int\n",
    "    ) -> Dict:\n",
    "        batch = {}\n",
    "        features = self.X.iloc[index, :].values.tolist()\n",
    "        features = torch.as_tensor(features)\n",
    "        \n",
    "        batch[\"features\"] = features\n",
    "        batch[\"labels\"] = self.y[index]\n",
    "        return batch\n",
    "\n",
    "def create_dataloader_pywt(\n",
    "    df: pd.DataFrame\n",
    ") -> DataLoader:\n",
    "    dataset = SER_Dataset_PYWT(df)\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=64,\n",
    "        num_workers=0,\n",
    "        drop_last=True\n",
    "    )\n",
    "    \n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f879ab29",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselinePYWT(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(\n",
    "                in_features=8,\n",
    "                out_features=64\n",
    "            ),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(\n",
    "                in_features=64,\n",
    "                out_features=128\n",
    "            ),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(\n",
    "                in_features=128,\n",
    "                out_features=3\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    def forward(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        return self.model(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2b9b9b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training epoch: 1/150\n",
      "Training loss: 0.985934\n",
      "Training F1: 0.264924\n",
      "\n",
      "Training epoch: 2/150\n",
      "Training loss: 0.744077\n",
      "Training F1: 0.292592\n",
      "\n",
      "Training epoch: 3/150\n",
      "Training loss: 0.675169\n",
      "Training F1: 0.292987\n",
      "\n",
      "Training epoch: 4/150\n",
      "Training loss: 0.650148\n",
      "Training F1: 0.292987\n",
      "\n",
      "Training epoch: 5/150\n",
      "Training loss: 0.639814\n",
      "Training F1: 0.292987\n",
      "\n",
      "Training epoch: 6/150\n",
      "Training loss: 0.632420\n",
      "Training F1: 0.292987\n",
      "\n",
      "Training epoch: 7/150\n",
      "Training loss: 0.624934\n",
      "Training F1: 0.292987\n",
      "\n",
      "Training epoch: 8/150\n",
      "Training loss: 0.620964\n",
      "Training F1: 0.292987\n",
      "\n",
      "Training epoch: 9/150\n",
      "Training loss: 0.617439\n",
      "Training F1: 0.301507\n",
      "\n",
      "Training epoch: 10/150\n",
      "Training loss: 0.613711\n",
      "Training F1: 0.301507\n",
      "\n",
      "Training epoch: 11/150\n",
      "Training loss: 0.609810\n",
      "Training F1: 0.301507\n",
      "\n",
      "Training epoch: 12/150\n",
      "Training loss: 0.606681\n",
      "Training F1: 0.310025\n",
      "\n",
      "Training epoch: 13/150\n",
      "Training loss: 0.603920\n",
      "Training F1: 0.310025\n",
      "\n",
      "Training epoch: 14/150\n",
      "Training loss: 0.600803\n",
      "Training F1: 0.310025\n",
      "\n",
      "Training epoch: 15/150\n",
      "Training loss: 0.600012\n",
      "Training F1: 0.315667\n",
      "\n",
      "Training epoch: 16/150\n",
      "Training loss: 0.597725\n",
      "Training F1: 0.310025\n",
      "\n",
      "Training epoch: 17/150\n",
      "Training loss: 0.593259\n",
      "Training F1: 0.323361\n",
      "\n",
      "Training epoch: 18/150\n",
      "Training loss: 0.592162\n",
      "Training F1: 0.335272\n",
      "\n",
      "Training epoch: 19/150\n",
      "Training loss: 0.589079\n",
      "Training F1: 0.341258\n",
      "\n",
      "Training epoch: 20/150\n",
      "Training loss: 0.585943\n",
      "Training F1: 0.330383\n",
      "\n",
      "Training epoch: 21/150\n",
      "Training loss: 0.586137\n",
      "Training F1: 0.330383\n",
      "\n",
      "Training epoch: 22/150\n",
      "Training loss: 0.581645\n",
      "Training F1: 0.330383\n",
      "\n",
      "Training epoch: 23/150\n",
      "Training loss: 0.579711\n",
      "Training F1: 0.338901\n",
      "\n",
      "Training epoch: 24/150\n",
      "Training loss: 0.579498\n",
      "Training F1: 0.342886\n",
      "\n",
      "Training epoch: 25/150\n",
      "Training loss: 0.576020\n",
      "Training F1: 0.360857\n",
      "\n",
      "Training epoch: 26/150\n",
      "Training loss: 0.573237\n",
      "Training F1: 0.360857\n",
      "\n",
      "Training epoch: 27/150\n",
      "Training loss: 0.572316\n",
      "Training F1: 0.349607\n",
      "\n",
      "Training epoch: 28/150\n",
      "Training loss: 0.571459\n",
      "Training F1: 0.353760\n",
      "\n",
      "Training epoch: 29/150\n",
      "Training loss: 0.568235\n",
      "Training F1: 0.356486\n",
      "\n",
      "Training epoch: 30/150\n",
      "Training loss: 0.567380\n",
      "Training F1: 0.360132\n",
      "\n",
      "Training epoch: 31/150\n",
      "Training loss: 0.564897\n",
      "Training F1: 0.359665\n",
      "\n",
      "Training epoch: 32/150\n",
      "Training loss: 0.562201\n",
      "Training F1: 0.371393\n",
      "\n",
      "Training epoch: 33/150\n",
      "Training loss: 0.560619\n",
      "Training F1: 0.367361\n",
      "\n",
      "Training epoch: 34/150\n",
      "Training loss: 0.562480\n",
      "Training F1: 0.353991\n",
      "\n",
      "Training epoch: 35/150\n",
      "Training loss: 0.557672\n",
      "Training F1: 0.371393\n",
      "\n",
      "Training epoch: 36/150\n",
      "Training loss: 0.556477\n",
      "Training F1: 0.373072\n",
      "\n",
      "Training epoch: 37/150\n",
      "Training loss: 0.556588\n",
      "Training F1: 0.361245\n",
      "\n",
      "Training epoch: 38/150\n",
      "Training loss: 0.552745\n",
      "Training F1: 0.373047\n",
      "\n",
      "Training epoch: 39/150\n",
      "Training loss: 0.553215\n",
      "Training F1: 0.377079\n",
      "\n",
      "Training epoch: 40/150\n",
      "Training loss: 0.548994\n",
      "Training F1: 0.377079\n",
      "\n",
      "Training epoch: 41/150\n",
      "Training loss: 0.546882\n",
      "Training F1: 0.377572\n",
      "\n",
      "Training epoch: 42/150\n",
      "Training loss: 0.545944\n",
      "Training F1: 0.370233\n",
      "\n",
      "Training epoch: 43/150\n",
      "Training loss: 0.543042\n",
      "Training F1: 0.383445\n",
      "\n",
      "Training epoch: 44/150\n",
      "Training loss: 0.544143\n",
      "Training F1: 0.377548\n",
      "\n",
      "Training epoch: 45/150\n",
      "Training loss: 0.546409\n",
      "Training F1: 0.371050\n",
      "\n",
      "Training epoch: 46/150\n",
      "Training loss: 0.541826\n",
      "Training F1: 0.378570\n",
      "\n",
      "Training epoch: 47/150\n",
      "Training loss: 0.541940\n",
      "Training F1: 0.368783\n",
      "\n",
      "Training epoch: 48/150\n",
      "Training loss: 0.540139\n",
      "Training F1: 0.370156\n",
      "\n",
      "Training epoch: 49/150\n",
      "Training loss: 0.534895\n",
      "Training F1: 0.397371\n",
      "\n",
      "Training epoch: 50/150\n",
      "Training loss: 0.534979\n",
      "Training F1: 0.378567\n",
      "\n",
      "Training epoch: 51/150\n",
      "Training loss: 0.534670\n",
      "Training F1: 0.391854\n",
      "\n",
      "Training epoch: 52/150\n",
      "Training loss: 0.528917\n",
      "Training F1: 0.397747\n",
      "\n",
      "Training epoch: 53/150\n",
      "Training loss: 0.530913\n",
      "Training F1: 0.380545\n",
      "\n",
      "Training epoch: 54/150\n",
      "Training loss: 0.527764\n",
      "Training F1: 0.382720\n",
      "\n",
      "Training epoch: 55/150\n",
      "Training loss: 0.524302\n",
      "Training F1: 0.404075\n",
      "\n",
      "Training epoch: 56/150\n",
      "Training loss: 0.525118\n",
      "Training F1: 0.398266\n",
      "\n",
      "Training epoch: 57/150\n",
      "Training loss: 0.521206\n",
      "Training F1: 0.396030\n",
      "\n",
      "Training epoch: 58/150\n",
      "Training loss: 0.516852\n",
      "Training F1: 0.413614\n",
      "\n",
      "Training epoch: 59/150\n",
      "Training loss: 0.521480\n",
      "Training F1: 0.406381\n",
      "\n",
      "Training epoch: 60/150\n",
      "Training loss: 0.516015\n",
      "Training F1: 0.432961\n",
      "\n",
      "Training epoch: 61/150\n",
      "Training loss: 0.510365\n",
      "Training F1: 0.427881\n",
      "\n",
      "Training epoch: 62/150\n",
      "Training loss: 0.514558\n",
      "Training F1: 0.394930\n",
      "\n",
      "Training epoch: 63/150\n",
      "Training loss: 0.512316\n",
      "Training F1: 0.412470\n",
      "\n",
      "Training epoch: 64/150\n",
      "Training loss: 0.514003\n",
      "Training F1: 0.427022\n",
      "\n",
      "Training epoch: 65/150\n",
      "Training loss: 0.504331\n",
      "Training F1: 0.446333\n",
      "\n",
      "Training epoch: 66/150\n",
      "Training loss: 0.512119\n",
      "Training F1: 0.411826\n",
      "\n",
      "Training epoch: 67/150\n",
      "Training loss: 0.513669\n",
      "Training F1: 0.435990\n",
      "\n",
      "Training epoch: 68/150\n",
      "Training loss: 0.518107\n",
      "Training F1: 0.406381\n",
      "\n",
      "Training epoch: 69/150\n",
      "Training loss: 0.505017\n",
      "Training F1: 0.453485\n",
      "\n",
      "Training epoch: 70/150\n",
      "Training loss: 0.502754\n",
      "Training F1: 0.430890\n",
      "\n",
      "Training epoch: 71/150\n",
      "Training loss: 0.510672\n",
      "Training F1: 0.409297\n",
      "\n",
      "Training epoch: 72/150\n",
      "Training loss: 0.499044\n",
      "Training F1: 0.457184\n",
      "\n",
      "Training epoch: 73/150\n",
      "Training loss: 0.497330\n",
      "Training F1: 0.475290\n",
      "\n",
      "Training epoch: 74/150\n",
      "Training loss: 0.497169\n",
      "Training F1: 0.462281\n",
      "\n",
      "Training epoch: 75/150\n",
      "Training loss: 0.497169\n",
      "Training F1: 0.453103\n",
      "\n",
      "Training epoch: 76/150\n",
      "Training loss: 0.488769\n",
      "Training F1: 0.494410\n",
      "\n",
      "Training epoch: 77/150\n",
      "Training loss: 0.484022\n",
      "Training F1: 0.493038\n",
      "\n",
      "Training epoch: 78/150\n",
      "Training loss: 0.488464\n",
      "Training F1: 0.466242\n",
      "\n",
      "Training epoch: 79/150\n",
      "Training loss: 0.489070\n",
      "Training F1: 0.463289\n",
      "\n",
      "Training epoch: 80/150\n",
      "Training loss: 0.485332\n",
      "Training F1: 0.498359\n",
      "\n",
      "Training epoch: 81/150\n",
      "Training loss: 0.481831\n",
      "Training F1: 0.482481\n",
      "\n",
      "Training epoch: 82/150\n",
      "Training loss: 0.477386\n",
      "Training F1: 0.508846\n",
      "\n",
      "Training epoch: 83/150\n",
      "Training loss: 0.482809\n",
      "Training F1: 0.480362\n",
      "\n",
      "Training epoch: 84/150\n",
      "Training loss: 0.479940\n",
      "Training F1: 0.489969\n",
      "\n",
      "Training epoch: 85/150\n",
      "Training loss: 0.477986\n",
      "Training F1: 0.496336\n",
      "\n",
      "Training epoch: 86/150\n",
      "Training loss: 0.466939\n",
      "Training F1: 0.534434\n",
      "\n",
      "Training epoch: 87/150\n",
      "Training loss: 0.470161\n",
      "Training F1: 0.523617\n",
      "\n",
      "Training epoch: 88/150\n",
      "Training loss: 0.471411\n",
      "Training F1: 0.503457\n",
      "\n",
      "Training epoch: 89/150\n",
      "Training loss: 0.478320\n",
      "Training F1: 0.495221\n",
      "\n",
      "Training epoch: 90/150\n",
      "Training loss: 0.461454\n",
      "Training F1: 0.495405\n",
      "\n",
      "Training epoch: 91/150\n",
      "Training loss: 0.466505\n",
      "Training F1: 0.498574\n",
      "\n",
      "Training epoch: 92/150\n",
      "Training loss: 0.463687\n",
      "Training F1: 0.522121\n",
      "\n",
      "Training epoch: 93/150\n",
      "Training loss: 0.459738\n",
      "Training F1: 0.531864\n",
      "\n",
      "Training epoch: 94/150\n",
      "Training loss: 0.455363\n",
      "Training F1: 0.544878\n",
      "\n",
      "Training epoch: 95/150\n",
      "Training loss: 0.453749\n",
      "Training F1: 0.562876\n",
      "\n",
      "Training epoch: 96/150\n",
      "Training loss: 0.450267\n",
      "Training F1: 0.537373\n",
      "\n",
      "Training epoch: 97/150\n",
      "Training loss: 0.442226\n",
      "Training F1: 0.529465\n",
      "\n",
      "Training epoch: 98/150\n",
      "Training loss: 0.449373\n",
      "Training F1: 0.566361\n",
      "\n",
      "Training epoch: 99/150\n",
      "Training loss: 0.446320\n",
      "Training F1: 0.567028\n",
      "\n",
      "Training epoch: 100/150\n",
      "Training loss: 0.444707\n",
      "Training F1: 0.544212\n",
      "\n",
      "Training epoch: 101/150\n",
      "Training loss: 0.445107\n",
      "Training F1: 0.523726\n",
      "\n",
      "Training epoch: 102/150\n",
      "Training loss: 0.440303\n",
      "Training F1: 0.540910\n",
      "\n",
      "Training epoch: 103/150\n",
      "Training loss: 0.433026\n",
      "Training F1: 0.574180\n",
      "\n",
      "Training epoch: 104/150\n",
      "Training loss: 0.439433\n",
      "Training F1: 0.562067\n",
      "\n",
      "Training epoch: 105/150\n",
      "Training loss: 0.441300\n",
      "Training F1: 0.544321\n",
      "\n",
      "Training epoch: 106/150\n",
      "Training loss: 0.466580\n",
      "Training F1: 0.497264\n",
      "\n",
      "Training epoch: 107/150\n",
      "Training loss: 0.464256\n",
      "Training F1: 0.516883\n",
      "\n",
      "Training epoch: 108/150\n",
      "Training loss: 0.481102\n",
      "Training F1: 0.506109\n",
      "\n",
      "Training epoch: 109/150\n",
      "Training loss: 0.470108\n",
      "Training F1: 0.489174\n",
      "\n",
      "Training epoch: 110/150\n",
      "Training loss: 0.455452\n",
      "Training F1: 0.555280\n",
      "\n",
      "Training epoch: 111/150\n",
      "Training loss: 0.448690\n",
      "Training F1: 0.534953\n",
      "\n",
      "Training epoch: 112/150\n",
      "Training loss: 0.443053\n",
      "Training F1: 0.501621\n",
      "\n",
      "Training epoch: 113/150\n",
      "Training loss: 0.436389\n",
      "Training F1: 0.544183\n",
      "\n",
      "Training epoch: 114/150\n",
      "Training loss: 0.430484\n",
      "Training F1: 0.574763\n",
      "\n",
      "Training epoch: 115/150\n",
      "Training loss: 0.426183\n",
      "Training F1: 0.571400\n",
      "\n",
      "Training epoch: 116/150\n",
      "Training loss: 0.423365\n",
      "Training F1: 0.570170\n",
      "\n",
      "Training epoch: 117/150\n",
      "Training loss: 0.428257\n",
      "Training F1: 0.554188\n",
      "\n",
      "Training epoch: 118/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.425387\n",
      "Training F1: 0.519810\n",
      "\n",
      "Training epoch: 119/150\n",
      "Training loss: 0.429469\n",
      "Training F1: 0.557568\n",
      "\n",
      "Training epoch: 120/150\n",
      "Training loss: 0.428940\n",
      "Training F1: 0.550299\n",
      "\n",
      "Training epoch: 121/150\n",
      "Training loss: 0.424570\n",
      "Training F1: 0.585718\n",
      "\n",
      "Training epoch: 122/150\n",
      "Training loss: 0.424342\n",
      "Training F1: 0.558216\n",
      "\n",
      "Training epoch: 123/150\n",
      "Training loss: 0.429889\n",
      "Training F1: 0.557096\n",
      "\n",
      "Training epoch: 124/150\n",
      "Training loss: 0.422810\n",
      "Training F1: 0.531556\n",
      "\n",
      "Training epoch: 125/150\n",
      "Training loss: 0.420247\n",
      "Training F1: 0.575299\n",
      "\n",
      "Training epoch: 126/150\n",
      "Training loss: 0.424688\n",
      "Training F1: 0.557103\n",
      "\n",
      "Training epoch: 127/150\n",
      "Training loss: 0.418997\n",
      "Training F1: 0.550732\n",
      "\n",
      "Training epoch: 128/150\n",
      "Training loss: 0.421119\n",
      "Training F1: 0.593124\n",
      "\n",
      "Training epoch: 129/150\n",
      "Training loss: 0.412850\n",
      "Training F1: 0.581460\n",
      "\n",
      "Training epoch: 130/150\n",
      "Training loss: 0.410699\n",
      "Training F1: 0.594812\n",
      "\n",
      "Training epoch: 131/150\n",
      "Training loss: 0.417403\n",
      "Training F1: 0.577079\n",
      "\n",
      "Training epoch: 132/150\n",
      "Training loss: 0.410943\n",
      "Training F1: 0.589811\n",
      "\n",
      "Training epoch: 133/150\n",
      "Training loss: 0.407258\n",
      "Training F1: 0.601862\n",
      "\n",
      "Training epoch: 134/150\n",
      "Training loss: 0.404235\n",
      "Training F1: 0.612029\n",
      "\n",
      "Training epoch: 135/150\n",
      "Training loss: 0.399372\n",
      "Training F1: 0.620105\n",
      "\n",
      "Training epoch: 136/150\n",
      "Training loss: 0.408864\n",
      "Training F1: 0.553106\n",
      "\n",
      "Training epoch: 137/150\n",
      "Training loss: 0.397202\n",
      "Training F1: 0.599700\n",
      "\n",
      "Training epoch: 138/150\n",
      "Training loss: 0.390715\n",
      "Training F1: 0.630064\n",
      "\n",
      "Training epoch: 139/150\n",
      "Training loss: 0.393267\n",
      "Training F1: 0.609289\n",
      "\n",
      "Training epoch: 140/150\n",
      "Training loss: 0.387384\n",
      "Training F1: 0.623217\n",
      "\n",
      "Training epoch: 141/150\n",
      "Training loss: 0.379192\n",
      "Training F1: 0.620936\n",
      "\n",
      "Training epoch: 142/150\n",
      "Training loss: 0.383299\n",
      "Training F1: 0.619606\n",
      "\n",
      "Training epoch: 143/150\n",
      "Training loss: 0.390904\n",
      "Training F1: 0.620963\n",
      "\n",
      "Training epoch: 144/150\n",
      "Training loss: 0.406429\n",
      "Training F1: 0.569971\n",
      "\n",
      "Training epoch: 145/150\n",
      "Training loss: 0.402966\n",
      "Training F1: 0.570231\n",
      "\n",
      "Training epoch: 146/150\n",
      "Training loss: 0.394251\n",
      "Training F1: 0.625165\n",
      "\n",
      "Training epoch: 147/150\n",
      "Training loss: 0.388671\n",
      "Training F1: 0.631892\n",
      "\n",
      "Training epoch: 148/150\n",
      "Training loss: 0.379100\n",
      "Training F1: 0.618220\n",
      "\n",
      "Training epoch: 149/150\n",
      "Training loss: 0.370406\n",
      "Training F1: 0.630680\n",
      "\n",
      "Training epoch: 150/150\n",
      "Training loss: 0.366275\n",
      "Training F1: 0.655260\n"
     ]
    }
   ],
   "source": [
    "wavelet_df[\"target\"] = wavelet_df[\"target\"].replace({\n",
    "    \"neutral\": 0,\n",
    "    \"non-neutral-male\": 1,\n",
    "    \"non-neutral-female\": 2\n",
    "})\n",
    "dataloader = create_dataloader_pywt(wavelet_df)\n",
    "device = torch.device(\"cpu\")\n",
    "model = BaselinePYWT().to(device)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.001)\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "epochs = 150\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    print(f\"\\nTraining epoch: {epoch}/{epochs}\")\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_f1 = 0.0\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        data = batch[\"features\"].to(device)\n",
    "        target = batch[\"labels\"].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(data)\n",
    "        \n",
    "        l = loss(output, target)\n",
    "        train_loss += l.item()\n",
    "        \n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        prediction = output.argmax(dim=-1, keepdim=True)\n",
    "        train_f1 += f1_score(\n",
    "            target.detach().cpu().numpy(),\n",
    "            prediction.detach().cpu().numpy(),\n",
    "            average=\"macro\"\n",
    "        )\n",
    "        \n",
    "    train_loss /= len(dataloader)\n",
    "    train_f1 /= len(dataloader)\n",
    "    \n",
    "    print(f\"Training loss: {train_loss:1.6f}\")\n",
    "    print(f\"Training F1: {train_f1:1.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3021c3",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6c4e17d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_distribute(\n",
    "    data: np.ndarray, sequence_length: int, stride: int = None, z_pad: bool = True\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Takes a sequence of features or labels and creates an np.ndarray of time\n",
    "    distributed sequences for input to a Keras TimeDistributed() layer.\n",
    "\n",
    "    Args:\n",
    "        data: The array to be time distributed.\n",
    "        sequence_length: The length of the output sequences in samples.\n",
    "        stride (optional): The number of samples between sequences. Defaults to sequence_length.\n",
    "        z_pad (optional): Zero padding to ensure all sequences to have the same dimensions.\n",
    "        Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        The time ditributed data sequences.\n",
    "\n",
    "    Example:\n",
    "        Given an np.ndarray of data:\n",
    "        >>> data.shape\n",
    "        (10000, 32, 32, 1)\n",
    "        >>> time_distribute(data, 10).shape\n",
    "        (1000, 10, 32, 32, 1)\n",
    "        The function yeilds 1000 training sequences, each 10 samples long.\n",
    "    \"\"\"\n",
    "\n",
    "    if stride is None:\n",
    "        stride = sequence_length\n",
    "        \n",
    "    if stride > sequence_length:\n",
    "        print(\n",
    "            \"WARNING: Stride longer than sequence length, causing missed samples. This is not recommended.\"\n",
    "        )\n",
    "    \n",
    "    td_data = []\n",
    "    \n",
    "    for n in range(0, len(data) - sequence_length + 1, stride):\n",
    "        td_data.append(data[n : n + sequence_length])\n",
    "    \n",
    "    if z_pad:\n",
    "        if len(td_data) * stride + sequence_length != len(data) + stride:\n",
    "            z_needed = len(td_data) * stride + sequence_length - len(data)\n",
    "            z_padded = np.zeros(td_data[0].shape)\n",
    "            for i in range(sequence_length - z_needed):\n",
    "                z_padded[i] = data[-(sequence_length - z_needed) + i]\n",
    "            td_data.append(z_padded)\n",
    "    \n",
    "    return np.array(td_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bc09c869",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>label</th>\n",
       "      <th>wav_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bpubdl02_segment247_non-neutral-male.wav</td>\n",
       "      <td>1</td>\n",
       "      <td>bpubdl02_segment247_non-neutral-male.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bpubmn14_segment89_neutral.wav</td>\n",
       "      <td>0</td>\n",
       "      <td>bpubmn14_segment89_neutral.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bfamdl26_segment93_neutral.wav</td>\n",
       "      <td>0</td>\n",
       "      <td>bfamdl26_segment93_neutral.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bfammn27_segment275_neutral.wav</td>\n",
       "      <td>0</td>\n",
       "      <td>bfammn27_segment275_neutral.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bfamcv02_segment276_neutral.wav</td>\n",
       "      <td>0</td>\n",
       "      <td>bfamcv02_segment276_neutral.wav</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       file  label  \\\n",
       "0  bpubdl02_segment247_non-neutral-male.wav      1   \n",
       "1            bpubmn14_segment89_neutral.wav      0   \n",
       "2            bfamdl26_segment93_neutral.wav      0   \n",
       "3           bfammn27_segment275_neutral.wav      0   \n",
       "4           bfamcv02_segment276_neutral.wav      0   \n",
       "\n",
       "                                   wav_file  \n",
       "0  bpubdl02_segment247_non-neutral-male.wav  \n",
       "1            bpubmn14_segment89_neutral.wav  \n",
       "2            bfamdl26_segment93_neutral.wav  \n",
       "3           bfammn27_segment275_neutral.wav  \n",
       "4           bfamcv02_segment276_neutral.wav  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "propor_path = \"/media/greca/HD/Datasets/PROPOR 2022/data_train/train\"\n",
    "df = create_propor_train_dataframe(propor_path)\n",
    "df[\"label\"] = df[\"label\"].replace({\n",
    "    \"neutral\": 0,\n",
    "    \"non-neutral-male\": 1,\n",
    "    \"non-neutral-female\": 2\n",
    "})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bf87565c",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to load audio from bpubdl02_segment247_non-neutral-male.wav",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchaudio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m transforms\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m stats\n\u001b[0;32m----> 4\u001b[0m audio, sr \u001b[38;5;241m=\u001b[39m \u001b[43mread_audio\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfile\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mto_mono\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16000\u001b[39;49m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m coeffs \u001b[38;5;241m=\u001b[39m pywt\u001b[38;5;241m.\u001b[39mwavedec(\n\u001b[1;32m     11\u001b[0m     audio\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mnumpy(),\n\u001b[1;32m     12\u001b[0m     wavelet\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdb4\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m     axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     16\u001b[0m )\n\u001b[1;32m     18\u001b[0m transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mMFCC(\n\u001b[1;32m     19\u001b[0m     sample_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16000\u001b[39m,\n\u001b[1;32m     20\u001b[0m     n_mfcc\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m13\u001b[39m\n\u001b[1;32m     21\u001b[0m )\n",
      "Cell \u001b[0;32mIn[18], line 33\u001b[0m, in \u001b[0;36mread_audio\u001b[0;34m(path, to_mono, sample_rate)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_audio\u001b[39m(\n\u001b[1;32m     17\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m     18\u001b[0m     to_mono: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     19\u001b[0m     sample_rate: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     20\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;28mint\u001b[39m]:\n\u001b[1;32m     21\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;124;03m    Reads a audio file.\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;03m        Tuple[torch.Tensor, int]: the audio waveform and the sample rate.\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m     audio, sr \u001b[38;5;241m=\u001b[39m \u001b[43mtorchaudio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sample_rate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m sample_rate \u001b[38;5;241m!=\u001b[39m sr:\n\u001b[1;32m     36\u001b[0m         audio \u001b[38;5;241m=\u001b[39m resample_audio(\n\u001b[1;32m     37\u001b[0m             audio\u001b[38;5;241m=\u001b[39maudio,\n\u001b[1;32m     38\u001b[0m             sample_rate\u001b[38;5;241m=\u001b[39msr,\n\u001b[1;32m     39\u001b[0m             new_sample_rate\u001b[38;5;241m=\u001b[39msample_rate\n\u001b[1;32m     40\u001b[0m         )\n",
      "File \u001b[0;32m/media/greca/HD/anaconda3/envs/ser/lib/python3.8/site-packages/torchaudio/backend/sox_io_backend.py:246\u001b[0m, in \u001b[0;36mload\u001b[0;34m(filepath, frame_offset, num_frames, normalize, channels_first, format)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\n\u001b[0;32m--> 246\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_fallback_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_offset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannels_first\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/greca/HD/anaconda3/envs/ser/lib/python3.8/site-packages/torchaudio/backend/sox_io_backend.py:30\u001b[0m, in \u001b[0;36m_fail_load\u001b[0;34m(filepath, frame_offset, num_frames, normalize, channels_first, format)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fail_load\u001b[39m(\n\u001b[1;32m     23\u001b[0m     filepath: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m     24\u001b[0m     frame_offset: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28mformat\u001b[39m: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     29\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;28mint\u001b[39m]:\n\u001b[0;32m---> 30\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to load audio from \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(filepath))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to load audio from bpubdl02_segment247_non-neutral-male.wav"
     ]
    }
   ],
   "source": [
    "from torchaudio import transforms\n",
    "from scipy import stats\n",
    "\n",
    "audio, sr = read_audio(\n",
    "    path=df[\"file\"][0],\n",
    "    to_mono=True,\n",
    "    sample_rate=16000\n",
    ")\n",
    "\n",
    "coeffs = pywt.wavedec(\n",
    "    audio.squeeze(0).numpy(),\n",
    "    wavelet=\"db4\",\n",
    "    mode='symmetric',\n",
    "    level=4,\n",
    "    axis=-1\n",
    ")\n",
    "\n",
    "transform = transforms.MFCC(\n",
    "    sample_rate=16000,\n",
    "    n_mfcc=13\n",
    ")\n",
    "\n",
    "means = []\n",
    "variances = []\n",
    "\n",
    "for i in range(len(coeffs)):\n",
    "    coeff = coeffs[i]\n",
    "    coeff = time_distribute(coeff, 400)\n",
    "    mfcc_coeff = transform(torch.from_numpy(coeff).to(dtype=torch.float32))\n",
    "    \n",
    "    means_mfcc_coeff = torch.mean(mfcc_coeff, dim=(-2, -1))\n",
    "    means.append(torch.mean(means_mfcc_coeff))\n",
    "    \n",
    "    variances_mfcc_coeff = torch.var(mfcc_coeff, dim=(-2, -1))\n",
    "    variances.append(torch.mean(variances_mfcc_coeff))\n",
    "    \n",
    "    print(mfcc_coeff.shape, stats.skew(mfcc_coeff, axis=(-2, -1)).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7e7e17",
   "metadata": {},
   "source": [
    "## Test 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ca0febb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_raw_audio(\n",
    "    features: List,\n",
    "    max_frames: int\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Auxiliary function to pad the features.\n",
    "    \n",
    "    Args:\n",
    "        features (List): the features that will be padded (mfcc, spectogram or mel_spectogram).\n",
    "        max_frames (int): the max frames value.\n",
    "    \n",
    "    Returns:\n",
    "        List: the padded features.\n",
    "    \"\"\"\n",
    "    features = [\n",
    "        F.pad(f, (0, max_frames - f.size(1)))\n",
    "        for f in features\n",
    "    ]\n",
    "    return features\n",
    "\n",
    "def find_max_dimensions_raw_audio(\n",
    "    df: pd.DataFrame,\n",
    "    params: Dict\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    Workaround to find the maximum dimension of the raw audio (number of frames).\n",
    "\n",
    "    Args:\n",
    "        audios (List): List of audio waveforms.\n",
    "\n",
    "    Returns:\n",
    "        int: the max number of frames.\n",
    "    \"\"\"\n",
    "    audios = []\n",
    "    \n",
    "    for file_path in df[\"file\"]:\n",
    "        audio, _ = read_audio(\n",
    "            path=file_path,\n",
    "            to_mono=params[\"feature\"][\"to_mono\"],\n",
    "            sample_rate=params[\"feature\"][\"sample_rate\"]\n",
    "        )\n",
    "        audios.append(audio)\n",
    "        \n",
    "    max_frames = max([x.size(1) for x in audios])\n",
    "    return max_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b71a2c31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>label</th>\n",
       "      <th>wav_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/media/greca/HD/Datasets/PROPOR 2022/data_trai...</td>\n",
       "      <td>1</td>\n",
       "      <td>bpubdl02_segment247_non-neutral-male.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/media/greca/HD/Datasets/PROPOR 2022/data_trai...</td>\n",
       "      <td>0</td>\n",
       "      <td>bpubmn14_segment89_neutral.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/media/greca/HD/Datasets/PROPOR 2022/data_trai...</td>\n",
       "      <td>0</td>\n",
       "      <td>bfamdl26_segment93_neutral.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/media/greca/HD/Datasets/PROPOR 2022/data_trai...</td>\n",
       "      <td>0</td>\n",
       "      <td>bfammn27_segment275_neutral.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/media/greca/HD/Datasets/PROPOR 2022/data_trai...</td>\n",
       "      <td>0</td>\n",
       "      <td>bfamcv02_segment276_neutral.wav</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                file  label  \\\n",
       "0  /media/greca/HD/Datasets/PROPOR 2022/data_trai...      1   \n",
       "1  /media/greca/HD/Datasets/PROPOR 2022/data_trai...      0   \n",
       "2  /media/greca/HD/Datasets/PROPOR 2022/data_trai...      0   \n",
       "3  /media/greca/HD/Datasets/PROPOR 2022/data_trai...      0   \n",
       "4  /media/greca/HD/Datasets/PROPOR 2022/data_trai...      0   \n",
       "\n",
       "                                   wav_file  \n",
       "0  bpubdl02_segment247_non-neutral-male.wav  \n",
       "1            bpubmn14_segment89_neutral.wav  \n",
       "2            bfamdl26_segment93_neutral.wav  \n",
       "3           bfammn27_segment275_neutral.wav  \n",
       "4           bfamcv02_segment276_neutral.wav  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "propor_path = \"/media/greca/HD/Datasets/PROPOR 2022/data_train/train\"\n",
    "df = create_propor_train_dataframe(propor_path)\n",
    "df[\"label\"] = df[\"label\"].replace({\n",
    "    \"neutral\": 0,\n",
    "    \"non-neutral-male\": 1,\n",
    "    \"non-neutral-female\": 2\n",
    "})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "0c1ee99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_rate = 16000\n",
    "n_mels = 60\n",
    "audios = []\n",
    "labels = []\n",
    "\n",
    "for file, label in zip(df[\"file\"], df[\"label\"]):\n",
    "    audio, sr = read_audio(\n",
    "        path=file,\n",
    "        to_mono=True,\n",
    "        sample_rate=sample_rate\n",
    "    )\n",
    "\n",
    "    labels.append(label)\n",
    "    audios.append(audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5f1df877",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([625, 224512])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_size = find_max_dimensions_raw_audio(df, params={\"feature\": {\"to_mono\": True, \"sample_rate\": sample_rate}})\n",
    "padded_audio = pad_raw_audio(audios, max_size)\n",
    "padded_audio = torch.concat(padded_audio, dim=0)\n",
    "padded_audio.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6c8439eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([625, 5, 60, 439])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "specs = []\n",
    "\n",
    "for i in range(padded_audio.shape[0]):\n",
    "\n",
    "    coeffs = pywt.wavedec(\n",
    "        padded_audio[i].squeeze().numpy(),\n",
    "        wavelet=\"db4\",\n",
    "        level=4,\n",
    "        mode=\"symmetric\",\n",
    "    )\n",
    "\n",
    "    new_coeffs = [torch.from_numpy(coeffs[i]).unsqueeze(0) for i in range(len(coeffs))]\n",
    "\n",
    "    transform = torchaudio.transforms.MelSpectrogram(\n",
    "        sample_rate=sample_rate,\n",
    "        n_mels=n_mels,\n",
    "        n_fft=512,\n",
    "        hop_length=256,\n",
    "        power=1\n",
    "    )\n",
    "#     transform = transforms.MFCC(\n",
    "#         sample_rate=sample_rate,\n",
    "#         n_mfcc=13,\n",
    "#         melkwargs={\"n_fft\": 512, \"hop_length\": 256, \"n_mels\": n_mels, \"center\": False}\n",
    "#     )\n",
    "\n",
    "    mel_specs = [transform(coeff) for coeff in new_coeffs]\n",
    "\n",
    "    max_height = max([x.size(1) for x in mel_specs])\n",
    "    max_width = max([x.size(2) for x in mel_specs])\n",
    "\n",
    "    mel_specs = pad_features(\n",
    "        features=mel_specs,\n",
    "        max_height=max_height,\n",
    "        max_width=max_width\n",
    "    )\n",
    "    mel_specs = torch.concat(mel_specs, dim=0).unsqueeze(0)\n",
    "    specs.append(mel_specs)\n",
    "\n",
    "specs = torch.concat(specs, dim=0)\n",
    "specs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a96a89a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineTeste2(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=5,\n",
    "                out_channels=64,\n",
    "                kernel_size=(2, 2),\n",
    "                padding=\"valid\"\n",
    "            ),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(\n",
    "                kernel_size=(2, 2)\n",
    "            ),\n",
    "            nn.Conv2d(\n",
    "                in_channels=64,\n",
    "                out_channels=128,\n",
    "                kernel_size=(2, 2),\n",
    "                padding=\"valid\"\n",
    "            ),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(\n",
    "                kernel_size=(2, 2)\n",
    "            ),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(\n",
    "                in_features=195328,\n",
    "                out_features=3\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    def forward(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        return self.model(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e7ae2636",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training epoch: 1/30\n",
      "Training loss: 5.010028\n",
      "Training F1: 0.281255\n",
      "\n",
      "Training epoch: 2/30\n",
      "Training loss: 1.694842\n",
      "Training F1: 0.394194\n",
      "\n",
      "Training epoch: 3/30\n",
      "Training loss: 1.243173\n",
      "Training F1: 0.403906\n",
      "\n",
      "Training epoch: 4/30\n",
      "Training loss: 0.992872\n",
      "Training F1: 0.555003\n",
      "\n",
      "Training epoch: 5/30\n",
      "Training loss: 0.844646\n",
      "Training F1: 0.552987\n",
      "\n",
      "Training epoch: 6/30\n",
      "Training loss: 1.172102\n",
      "Training F1: 0.534522\n",
      "\n",
      "Training epoch: 7/30\n",
      "Training loss: 0.868625\n",
      "Training F1: 0.613065\n",
      "\n",
      "Training epoch: 8/30\n",
      "Training loss: 0.823009\n",
      "Training F1: 0.624471\n",
      "\n",
      "Training epoch: 9/30\n",
      "Training loss: 0.743554\n",
      "Training F1: 0.652289\n",
      "\n",
      "Training epoch: 10/30\n",
      "Training loss: 0.568447\n",
      "Training F1: 0.703164\n",
      "\n",
      "Training epoch: 11/30\n",
      "Training loss: 0.427177\n",
      "Training F1: 0.693793\n",
      "\n",
      "Training epoch: 12/30\n",
      "Training loss: 0.343550\n",
      "Training F1: 0.765916\n",
      "\n",
      "Training epoch: 13/30\n",
      "Training loss: 0.296858\n",
      "Training F1: 0.805410\n",
      "\n",
      "Training epoch: 14/30\n",
      "Training loss: 0.306968\n",
      "Training F1: 0.792278\n",
      "\n",
      "Training epoch: 15/30\n",
      "Training loss: 0.346352\n",
      "Training F1: 0.770279\n",
      "\n",
      "Training epoch: 16/30\n",
      "Training loss: 0.394144\n",
      "Training F1: 0.769387\n",
      "\n",
      "Training epoch: 17/30\n",
      "Training loss: 0.395051\n",
      "Training F1: 0.737783\n",
      "\n",
      "Training epoch: 18/30\n",
      "Training loss: 0.376667\n",
      "Training F1: 0.775496\n",
      "\n",
      "Training epoch: 19/30\n",
      "Training loss: 0.437189\n",
      "Training F1: 0.750883\n",
      "\n",
      "Training epoch: 20/30\n",
      "Training loss: 0.319211\n",
      "Training F1: 0.796751\n",
      "\n",
      "Training epoch: 21/30\n",
      "Training loss: 0.281739\n",
      "Training F1: 0.801596\n",
      "\n",
      "Training epoch: 22/30\n",
      "Training loss: 0.392772\n",
      "Training F1: 0.802062\n",
      "\n",
      "Training epoch: 23/30\n",
      "Training loss: 0.205017\n",
      "Training F1: 0.856667\n",
      "\n",
      "Training epoch: 24/30\n",
      "Training loss: 0.202269\n",
      "Training F1: 0.832526\n",
      "\n",
      "Training epoch: 25/30\n",
      "Training loss: 0.260555\n",
      "Training F1: 0.824687\n",
      "\n",
      "Training epoch: 26/30\n",
      "Training loss: 0.287991\n",
      "Training F1: 0.868615\n",
      "\n",
      "Training epoch: 27/30\n",
      "Training loss: 0.172581\n",
      "Training F1: 0.902759\n",
      "\n",
      "Training epoch: 28/30\n",
      "Training loss: 0.340377\n",
      "Training F1: 0.866558\n",
      "\n",
      "Training epoch: 29/30\n",
      "Training loss: 0.215478\n",
      "Training F1: 0.897009\n",
      "\n",
      "Training epoch: 30/30\n",
      "Training loss: 0.293922\n",
      "Training F1: 0.849380\n"
     ]
    }
   ],
   "source": [
    "dataloader = create_dataloader_pth(specs, labels)\n",
    "device = torch.device(\"cpu\")\n",
    "model = BaselineTeste2().to(device)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.001)\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "epochs = 30\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    print(f\"\\nTraining epoch: {epoch}/{epochs}\")\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_f1 = 0.0\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        data = batch[\"features\"].to(device)\n",
    "        target = batch[\"labels\"].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(data)\n",
    "        \n",
    "        l = loss(output, target)\n",
    "        train_loss += l.item()\n",
    "        \n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        prediction = output.argmax(dim=-1, keepdim=True)\n",
    "        train_f1 += f1_score(\n",
    "            target.detach().cpu().numpy(),\n",
    "            prediction.detach().cpu().numpy(),\n",
    "            average=\"macro\"\n",
    "        )\n",
    "        \n",
    "    train_loss /= len(dataloader)\n",
    "    train_f1 /= len(dataloader)\n",
    "    \n",
    "    print(f\"Training loss: {train_loss:1.6f}\")\n",
    "    print(f\"Training F1: {train_f1:1.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5be3a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
