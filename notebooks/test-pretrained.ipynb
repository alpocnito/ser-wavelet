{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b0e4d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(\n",
    "                  os.path.dirname(\"test-pretrained\"), \n",
    "                  os.pardir)\n",
    ")\n",
    "sys.path.append(PROJECT_ROOT)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import json\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from copy import deepcopy\n",
    "from src.data_augmentation import Mixup, Specmix, Cutmix\n",
    "from src.features import extract_wavelet_from_raw_audio\n",
    "from src.dataset import create_dataloader\n",
    "from src.utils import feature_extraction_pipeline, read_features_files, choose_model, read_feature, pad_features\n",
    "from src.models.utils import SaveBestModel, weight_init\n",
    "from src.models.cnn3 import *\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from typing import Dict, Tuple, List, Union, Iterable\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "752cf685",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model: nn.Module,\n",
    "    dataloader: DataLoader,\n",
    "    optimizer: torch.optim.Adam,\n",
    "    loss: torch.nn.CrossEntropyLoss,\n",
    "    device: torch.device,\n",
    "    mixer: Union[None, Mixup, Specmix, Cutmix]\n",
    ") -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Function responsible for the model training.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): the created model.\n",
    "        dataloader (DataLoader): the training dataloader.\n",
    "        optimizer (torch.optim.Adam): the optimizer used.\n",
    "        loss (torch.nn.CrossEntropyLoss): the loss function used.\n",
    "        device (torch.device): which device to use.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[float, float]: the training f1 and loss, respectively.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    predictions = []\n",
    "    targets = []\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    for index, (batch) in enumerate(dataloader, start=1):\n",
    "        data = batch[\"features\"].to(device)\n",
    "        target = batch[\"labels\"].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        data = data.to(dtype=torch.float32)\n",
    "        target = target.to(dtype=torch.float32)\n",
    "        \n",
    "        if not mixer is None:\n",
    "            data, target = mixer(\n",
    "                x=data,\n",
    "                y=target\n",
    "            )\n",
    "            \n",
    "        output = model(data)\n",
    "\n",
    "        l = loss(output, target)\n",
    "        train_loss += l.item()\n",
    "        \n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        prediction = output.argmax(dim=-1, keepdim=True).to(dtype=torch.int)\n",
    "        prediction = prediction.detach().cpu().numpy()\n",
    "        predictions.extend(prediction.tolist())\n",
    "        \n",
    "        target = target.argmax(dim=-1, keepdim=True).to(dtype=torch.int)\n",
    "        target = target.detach().cpu().numpy()\n",
    "        targets.extend(target.tolist())\n",
    "        \n",
    "    train_loss = train_loss/index\n",
    "    train_f1 = classification_report(\n",
    "        targets,\n",
    "        predictions,\n",
    "        digits=6,\n",
    "        output_dict=True,\n",
    "        zero_division=0.0\n",
    "    )\n",
    "    train_f1 = train_f1[\"macro avg\"][\"f1-score\"]\n",
    "    return train_f1, train_loss\n",
    "\n",
    "def evaluate(\n",
    "    model: nn.Module,\n",
    "    dataloader: DataLoader,\n",
    "    loss: torch.nn.CrossEntropyLoss,\n",
    "    device: torch.device\n",
    ") -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Function responsible for the model evaluation.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): the created model.\n",
    "        dataloader (DataLoader): the validaiton dataloader.\n",
    "        loss (torch.nn.CrossEntropyLoss): the loss function used.\n",
    "        device (torch.device): which device to use.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[float, float]: the validation f1 and loss, respectively.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    targets = []\n",
    "    validation_loss = 0.0\n",
    "    validation_f1 = []\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        for index, (batch) in enumerate(dataloader):\n",
    "            data = batch[\"features\"].to(device)\n",
    "            target = batch[\"labels\"].to(device)\n",
    "\n",
    "            data = data.to(dtype=torch.float32)\n",
    "            target = target.to(dtype=torch.float32)\n",
    "                        \n",
    "            output = model(data)\n",
    "            \n",
    "            l = loss(output, target)\n",
    "            validation_loss += l.item()\n",
    "            \n",
    "            prediction = output.argmax(dim=-1, keepdim=True).to(dtype=torch.int)\n",
    "            prediction = prediction.detach().cpu().numpy()\n",
    "            predictions.extend(prediction.tolist())\n",
    "            \n",
    "            target = target.argmax(dim=-1, keepdim=True).to(dtype=torch.int)\n",
    "            target = target.detach().cpu().numpy()\n",
    "            targets.extend(target.tolist())\n",
    "    \n",
    "    validation_loss = validation_loss/index\n",
    "    validation_f1 = classification_report(\n",
    "        targets,\n",
    "        predictions,\n",
    "        digits=6,\n",
    "        output_dict=True,\n",
    "        zero_division=0.0\n",
    "    )\n",
    "    validation_f1 = validation_f1[\"macro avg\"][\"f1-score\"]\n",
    "    return validation_f1, validation_loss\n",
    "\n",
    "def test(\n",
    "    model: nn.Module,\n",
    "    dataloader: DataLoader,\n",
    "    device: torch.device\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Function responsible for the model testing in the test dataset.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): the created model.\n",
    "        dataloader (DataLoader): the test dataloader.\n",
    "        device (torch.device): which device to use.\n",
    "\n",
    "    Returns:\n",
    "        str: the test classification report.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    targets = []\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        for batch in dataloader:\n",
    "            data = batch[\"features\"].to(device)\n",
    "            target = batch[\"labels\"].to(device)\n",
    "\n",
    "            data = data.to(dtype=torch.float32)\n",
    "            target = target.to(dtype=torch.float32)\n",
    "            \n",
    "            output = model(data)\n",
    "            \n",
    "            prediction = output.argmax(dim=-1, keepdim=True).to(dtype=torch.int)\n",
    "            prediction = prediction.detach().cpu().numpy()\n",
    "            predictions.extend(prediction.tolist())\n",
    "            \n",
    "            target = target.argmax(dim=-1, keepdim=True).to(dtype=torch.int)\n",
    "            target = target.detach().cpu().numpy()\n",
    "            targets.extend(target.tolist())\n",
    "    \n",
    "    class_report = classification_report(\n",
    "        targets,\n",
    "        predictions,\n",
    "        digits=4,\n",
    "        output_dict=True\n",
    "    )\n",
    "    return class_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95a12db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: torch.Size([500, 1, 128000]), torch.Size([500, 3])\n",
      "Valid: torch.Size([125, 1, 128000]), torch.Size([125, 3])\n",
      "Test: torch.Size([308, 1, 128000]), torch.Size([308, 3])\n"
     ]
    }
   ],
   "source": [
    "features_path = \"../features8k/propor2022/\"\n",
    "\n",
    "# loading training features\n",
    "X_train = read_feature(path=features_path, fold=\"0\", name=\"X_train.pth\")\n",
    "y_train = read_feature(path=features_path, fold=\"0\", name=\"y_train.pth\")\n",
    "print(f\"Train: {X_train.shape}, {y_train.shape}\")\n",
    "\n",
    "# loading validation features\n",
    "X_valid = read_feature(path=features_path, fold=\"0\", name=\"X_valid.pth\")\n",
    "y_valid = read_feature(path=features_path, fold=\"0\", name=\"y_valid.pth\")\n",
    "print(f\"Valid: {X_valid.shape}, {y_valid.shape}\")\n",
    "\n",
    "# loading testing features\n",
    "X_test = read_feature(path=features_path, fold=None, name=\"X_test.pth\")\n",
    "y_test = read_feature(path=features_path, fold=None, name=\"y_test.pth\")\n",
    "print(f\"Test: {X_test.shape}, {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76c134a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/greca/HD/anaconda3/envs/ser/lib/python3.8/site-packages/librosa/util/decorators.py:88: UserWarning: Empty filters detected in mel frequency basis. Some channels will produce empty responses. Try increasing your sampling rate (and fmax) or reducing n_mels.\n",
      "  return f(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# reading the parameters configuration file\n",
    "params = json.load(open(\"../config/mode_1.json\", \"r\"))\n",
    "\n",
    "feature_config = params[\"feature\"]\n",
    "feature_config[\"sample_rate\"] = int(params[\"sample_rate\"])\n",
    "data_augmentation_config = params[\"data_augmentation\"]\n",
    "dataset = params[\"dataset\"]\n",
    "wavelet_config = params[\"wavelet\"]\n",
    "mode = params[\"mode\"]\n",
    "\n",
    "model_config = params[\"model\"]\n",
    "model_config[\"name\"] = \"cnn3\"\n",
    "model_config[\"use_gpu\"] = False\n",
    "model_config[\"learning_rate\"] = 0.0001\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available and model_config[\"use_gpu\"] else \"cpu\")\n",
    "\n",
    "if dataset == \"propor2022\":\n",
    "    if data_augmentation_config[\"target\"] == \"majority\":\n",
    "        data_augment_target = [0]\n",
    "    elif data_augmentation_config[\"target\"] == \"minority\":\n",
    "        data_augment_target = [1, 2]\n",
    "    elif data_augmentation_config[\"target\"] == \"all\":\n",
    "        data_augment_target = [0, 1, 2]\n",
    "    else:\n",
    "        raise ValueError(\"Invalid arguments for target. Should be 'all', 'majority' or 'minority\")\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "    \n",
    "model = choose_model(\n",
    "    mode=mode,\n",
    "    model_name=model_config[\"name\"],\n",
    "    device=device,\n",
    "    dataset=dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc39874b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoamOpt:\n",
    "    \"Optim wrapper that implements rate.\"\n",
    "    def __init__(self, model_size, factor, warmup, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        self._step = 0\n",
    "        self.warmup = warmup\n",
    "        self.factor = factor\n",
    "        self.model_size = model_size\n",
    "        self._rate = 0\n",
    "        \n",
    "    def step(self):\n",
    "        \"Update parameters and rate\"\n",
    "        self._step += 1\n",
    "        rate = self.rate()\n",
    "        for p in self.optimizer.param_groups:\n",
    "            p['lr'] = rate\n",
    "        self._rate = rate\n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def rate(self, step = None):\n",
    "        \"Implement `lrate` above\"\n",
    "        if step is None:\n",
    "            step = self._step\n",
    "        #return self.factor * \\ (self.model_size ** (-0.5) * min(step ** (-0.5), step * self.warmup ** (-1.5)))\n",
    "        return 0.0001\n",
    "        \n",
    "def get_std_opt(model):\n",
    "    return NoamOpt(model.src_embed[0].d_model, 2, 4000,\n",
    "            torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d187b9e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/100\n",
      "\n",
      "Epoch: 1\n",
      "Train F1-Score: 0.313523\n",
      "Train Loss: 1.811929\n",
      "Validation F1-Score: 0.300420\n",
      "Validation Loss: 1.825701\n",
      "Test F1-Score: 0.366102\n",
      "\n",
      "Epoch: 2/100\n",
      "\n",
      "Epoch: 2\n",
      "Train F1-Score: 0.339942\n",
      "Train Loss: 1.393618\n",
      "Validation F1-Score: 0.327557\n",
      "Validation Loss: 1.467577\n",
      "Test F1-Score: 0.371932\n",
      "\n",
      "Epoch: 3/100\n",
      "\n",
      "Epoch: 3\n",
      "Train F1-Score: 0.337208\n",
      "Train Loss: 1.337283\n",
      "Validation F1-Score: 0.291291\n",
      "Validation Loss: 1.607207\n",
      "Test F1-Score: 0.337789\n",
      "\n",
      "Epoch: 4/100\n",
      "\n",
      "Epoch: 4\n",
      "Train F1-Score: 0.321041\n",
      "Train Loss: 1.272540\n",
      "Validation F1-Score: 0.354012\n",
      "Validation Loss: 1.453305\n",
      "Test F1-Score: 0.326450\n",
      "\n",
      "Epoch: 5/100\n",
      "\n",
      "Epoch: 5\n",
      "Train F1-Score: 0.357263\n",
      "Train Loss: 1.075650\n",
      "Validation F1-Score: 0.316564\n",
      "Validation Loss: 1.259725\n",
      "Test F1-Score: 0.386010\n",
      "\n",
      "Epoch: 6/100\n",
      "\n",
      "Epoch: 6\n",
      "Train F1-Score: 0.332198\n",
      "Train Loss: 1.222494\n",
      "Validation F1-Score: 0.468241\n",
      "Validation Loss: 1.119771\n",
      "Test F1-Score: 0.337640\n",
      "\n",
      "Epoch: 7/100\n",
      "\n",
      "Epoch: 7\n",
      "Train F1-Score: 0.334489\n",
      "Train Loss: 1.149200\n",
      "Validation F1-Score: 0.289193\n",
      "Validation Loss: 1.765982\n",
      "Test F1-Score: 0.292355\n",
      "\n",
      "Epoch: 8/100\n",
      "\n",
      "Epoch: 8\n",
      "Train F1-Score: 0.325918\n",
      "Train Loss: 1.049822\n",
      "Validation F1-Score: 0.364112\n",
      "Validation Loss: 1.044441\n",
      "Test F1-Score: 0.320421\n",
      "\n",
      "Epoch: 9/100\n",
      "\n",
      "Epoch: 9\n",
      "Train F1-Score: 0.366510\n",
      "Train Loss: 0.856749\n",
      "Validation F1-Score: 0.348094\n",
      "Validation Loss: 1.160647\n",
      "Test F1-Score: 0.351036\n",
      "\n",
      "Epoch: 10/100\n",
      "\n",
      "Epoch: 10\n",
      "Train F1-Score: 0.346916\n",
      "Train Loss: 1.027308\n",
      "Validation F1-Score: 0.329027\n",
      "Validation Loss: 1.327446\n",
      "Test F1-Score: 0.319219\n",
      "\n",
      "Epoch: 11/100\n",
      "\n",
      "Epoch: 11\n",
      "Train F1-Score: 0.327476\n",
      "Train Loss: 1.016742\n",
      "Validation F1-Score: 0.420734\n",
      "Validation Loss: 0.881573\n",
      "Test F1-Score: 0.344156\n",
      "\n",
      "Epoch: 12/100\n",
      "\n",
      "Epoch: 12\n",
      "Train F1-Score: 0.327637\n",
      "Train Loss: 1.039094\n",
      "Validation F1-Score: 0.379260\n",
      "Validation Loss: 1.021314\n",
      "Test F1-Score: 0.349897\n",
      "\n",
      "Epoch: 13/100\n",
      "\n",
      "Epoch: 13\n",
      "Train F1-Score: 0.327370\n",
      "Train Loss: 0.927997\n",
      "Validation F1-Score: 0.355888\n",
      "Validation Loss: 1.235365\n",
      "Test F1-Score: 0.381091\n",
      "\n",
      "Epoch: 14/100\n",
      "\n",
      "Epoch: 14\n",
      "Train F1-Score: 0.339227\n",
      "Train Loss: 0.912408\n",
      "Validation F1-Score: 0.327028\n",
      "Validation Loss: 1.226924\n",
      "Test F1-Score: 0.336039\n",
      "\n",
      "Epoch: 15/100\n",
      "\n",
      "Epoch: 15\n",
      "Train F1-Score: 0.323545\n",
      "Train Loss: 0.915188\n",
      "Validation F1-Score: 0.354545\n",
      "Validation Loss: 1.066331\n",
      "Test F1-Score: 0.370097\n",
      "\n",
      "Epoch: 16/100\n",
      "\n",
      "Epoch: 16\n",
      "Train F1-Score: 0.328884\n",
      "Train Loss: 0.952614\n",
      "Validation F1-Score: 0.289193\n",
      "Validation Loss: 1.057720\n",
      "Test F1-Score: 0.331054\n",
      "\n",
      "Epoch: 17/100\n",
      "\n",
      "Epoch: 17\n",
      "Train F1-Score: 0.345155\n",
      "Train Loss: 0.870463\n",
      "Validation F1-Score: 0.322540\n",
      "Validation Loss: 1.119166\n",
      "Test F1-Score: 0.307862\n",
      "\n",
      "Epoch: 18/100\n",
      "\n",
      "Epoch: 18\n",
      "Train F1-Score: 0.389860\n",
      "Train Loss: 0.841810\n",
      "Validation F1-Score: 0.397380\n",
      "Validation Loss: 0.968220\n",
      "Test F1-Score: 0.307385\n",
      "\n",
      "Epoch: 19/100\n",
      "\n",
      "Epoch: 19\n",
      "Train F1-Score: 0.328129\n",
      "Train Loss: 0.882888\n",
      "Validation F1-Score: 0.401631\n",
      "Validation Loss: 0.925549\n",
      "Test F1-Score: 0.392515\n",
      "\n",
      "Epoch: 20/100\n",
      "\n",
      "Epoch: 20\n",
      "Train F1-Score: 0.327381\n",
      "Train Loss: 0.895808\n",
      "Validation F1-Score: 0.332385\n",
      "Validation Loss: 1.055838\n",
      "Test F1-Score: 0.324873\n",
      "\n",
      "Epoch: 21/100\n",
      "\n",
      "Epoch: 21\n",
      "Train F1-Score: 0.366313\n",
      "Train Loss: 0.852121\n",
      "Validation F1-Score: 0.292237\n",
      "Validation Loss: 1.171227\n",
      "Test F1-Score: 0.368398\n",
      "\n",
      "Epoch: 22/100\n",
      "\n",
      "Epoch: 22\n",
      "Train F1-Score: 0.357496\n",
      "Train Loss: 0.780166\n",
      "Validation F1-Score: 0.354607\n",
      "Validation Loss: 0.839329\n",
      "Test F1-Score: 0.387602\n",
      "\n",
      "Epoch: 23/100\n",
      "\n",
      "Epoch: 23\n",
      "Train F1-Score: 0.352144\n",
      "Train Loss: 0.861917\n",
      "Validation F1-Score: 0.477472\n",
      "Validation Loss: 0.885014\n",
      "Test F1-Score: 0.365833\n",
      "\n",
      "Epoch: 24/100\n",
      "\n",
      "Epoch: 24\n",
      "Train F1-Score: 0.316775\n",
      "Train Loss: 0.876386\n",
      "Validation F1-Score: 0.359276\n",
      "Validation Loss: 1.027070\n",
      "Test F1-Score: 0.357369\n",
      "\n",
      "Epoch: 25/100\n",
      "\n",
      "Epoch: 25\n",
      "Train F1-Score: 0.328404\n",
      "Train Loss: 0.794704\n",
      "Validation F1-Score: 0.327697\n",
      "Validation Loss: 1.070273\n",
      "Test F1-Score: 0.313658\n",
      "\n",
      "Epoch: 26/100\n",
      "\n",
      "Epoch: 26\n",
      "Train F1-Score: 0.347533\n",
      "Train Loss: 0.808747\n",
      "Validation F1-Score: 0.347793\n",
      "Validation Loss: 1.075403\n",
      "Test F1-Score: 0.385921\n",
      "\n",
      "Epoch: 27/100\n",
      "\n",
      "Epoch: 27\n",
      "Train F1-Score: 0.327434\n",
      "Train Loss: 0.760516\n",
      "Validation F1-Score: 0.348765\n",
      "Validation Loss: 1.031520\n",
      "Test F1-Score: 0.355238\n",
      "\n",
      "Epoch: 28/100\n",
      "\n",
      "Epoch: 28\n",
      "Train F1-Score: 0.349494\n",
      "Train Loss: 0.774875\n",
      "Validation F1-Score: 0.332385\n",
      "Validation Loss: 1.126819\n",
      "Test F1-Score: 0.330161\n",
      "\n",
      "Epoch: 29/100\n",
      "\n",
      "Epoch: 29\n",
      "Train F1-Score: 0.351117\n",
      "Train Loss: 0.784399\n",
      "Validation F1-Score: 0.374574\n",
      "Validation Loss: 1.096128\n",
      "Test F1-Score: 0.341359\n",
      "\n",
      "Epoch: 30/100\n",
      "\n",
      "Epoch: 30\n",
      "Train F1-Score: 0.399814\n",
      "Train Loss: 0.692151\n",
      "Validation F1-Score: 0.291291\n",
      "Validation Loss: 0.987350\n",
      "Test F1-Score: 0.291667\n",
      "\n",
      "Epoch: 31/100\n",
      "\n",
      "Epoch: 31\n",
      "Train F1-Score: 0.355963\n",
      "Train Loss: 0.735561\n",
      "Validation F1-Score: 0.330714\n",
      "Validation Loss: 0.929828\n",
      "Test F1-Score: 0.312018\n",
      "\n",
      "Epoch: 32/100\n",
      "\n",
      "Epoch: 32\n",
      "Train F1-Score: 0.398424\n",
      "Train Loss: 0.695126\n",
      "Validation F1-Score: 0.333001\n",
      "Validation Loss: 0.969698\n",
      "Test F1-Score: 0.328951\n",
      "\n",
      "Epoch: 33/100\n",
      "\n",
      "Epoch: 33\n",
      "Train F1-Score: 0.342608\n",
      "Train Loss: 0.727643\n",
      "Validation F1-Score: 0.329382\n",
      "Validation Loss: 0.921316\n",
      "Test F1-Score: 0.307170\n",
      "\n",
      "Epoch: 34/100\n",
      "\n",
      "Epoch: 34\n",
      "Train F1-Score: 0.383309\n",
      "Train Loss: 0.711482\n",
      "Validation F1-Score: 0.379948\n",
      "Validation Loss: 0.915656\n",
      "Test F1-Score: 0.373538\n",
      "\n",
      "Epoch: 35/100\n",
      "\n",
      "Epoch: 35\n",
      "Train F1-Score: 0.416821\n",
      "Train Loss: 0.704676\n",
      "Validation F1-Score: 0.325943\n",
      "Validation Loss: 0.914755\n",
      "Test F1-Score: 0.363345\n",
      "\n",
      "Epoch: 36/100\n",
      "\n",
      "Epoch: 36\n",
      "Train F1-Score: 0.348889\n",
      "Train Loss: 0.719507\n",
      "Validation F1-Score: 0.416143\n",
      "Validation Loss: 0.914123\n",
      "Test F1-Score: 0.353047\n",
      "\n",
      "Epoch: 37/100\n",
      "\n",
      "Epoch: 37\n",
      "Train F1-Score: 0.378819\n",
      "Train Loss: 0.719570\n",
      "Validation F1-Score: 0.332385\n",
      "Validation Loss: 0.947893\n",
      "Test F1-Score: 0.358851\n",
      "\n",
      "Epoch: 38/100\n",
      "\n",
      "Epoch: 38\n",
      "Train F1-Score: 0.360809\n",
      "Train Loss: 0.695351\n",
      "Validation F1-Score: 0.327028\n",
      "Validation Loss: 0.945010\n",
      "Test F1-Score: 0.338197\n",
      "\n",
      "Epoch: 39/100\n",
      "\n",
      "Epoch: 39\n",
      "Train F1-Score: 0.369726\n",
      "Train Loss: 0.728292\n",
      "Validation F1-Score: 0.463223\n",
      "Validation Loss: 1.000943\n",
      "Test F1-Score: 0.357485\n",
      "\n",
      "Epoch: 40/100\n",
      "\n",
      "Epoch: 40\n",
      "Train F1-Score: 0.361081\n",
      "Train Loss: 0.703387\n",
      "Validation F1-Score: 0.294643\n",
      "Validation Loss: 1.105265\n",
      "Test F1-Score: 0.295221\n",
      "\n",
      "Epoch: 41/100\n",
      "\n",
      "Epoch: 41\n",
      "Train F1-Score: 0.348129\n",
      "Train Loss: 0.732055\n",
      "Validation F1-Score: 0.352844\n",
      "Validation Loss: 0.872342\n",
      "Test F1-Score: 0.338578\n",
      "\n",
      "Epoch: 42/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/greca/HD/anaconda3/envs/ser/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/media/greca/HD/anaconda3/envs/ser/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/media/greca/HD/anaconda3/envs/ser/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 42\n",
      "Train F1-Score: 0.394556\n",
      "Train Loss: 0.723894\n",
      "Validation F1-Score: 0.294643\n",
      "Validation Loss: 0.986761\n",
      "Test F1-Score: 0.297232\n",
      "\n",
      "Epoch: 43/100\n",
      "\n",
      "Epoch: 43\n",
      "Train F1-Score: 0.368484\n",
      "Train Loss: 0.668103\n",
      "Validation F1-Score: 0.376409\n",
      "Validation Loss: 0.970149\n",
      "Test F1-Score: 0.396280\n",
      "\n",
      "Epoch: 44/100\n",
      "\n",
      "Epoch: 44\n",
      "Train F1-Score: 0.354194\n",
      "Train Loss: 0.732666\n",
      "Validation F1-Score: 0.292609\n",
      "Validation Loss: 1.062851\n",
      "Test F1-Score: 0.354321\n",
      "\n",
      "Epoch: 45/100\n",
      "\n",
      "Epoch: 45\n",
      "Train F1-Score: 0.410496\n",
      "Train Loss: 0.677237\n",
      "Validation F1-Score: 0.388032\n",
      "Validation Loss: 0.945430\n",
      "Test F1-Score: 0.322414\n",
      "\n",
      "Epoch: 46/100\n",
      "\n",
      "Epoch: 46\n",
      "Train F1-Score: 0.375210\n",
      "Train Loss: 0.718546\n",
      "Validation F1-Score: 0.333001\n",
      "Validation Loss: 1.078442\n",
      "Test F1-Score: 0.340050\n",
      "\n",
      "Epoch: 47/100\n",
      "\n",
      "Epoch: 47\n",
      "Train F1-Score: 0.379269\n",
      "Train Loss: 0.657019\n",
      "Validation F1-Score: 0.380535\n",
      "Validation Loss: 1.000965\n",
      "Test F1-Score: 0.322958\n",
      "\n",
      "Epoch: 48/100\n",
      "\n",
      "Epoch: 48\n",
      "Train F1-Score: 0.385117\n",
      "Train Loss: 0.680528\n",
      "Validation F1-Score: 0.423909\n",
      "Validation Loss: 0.915516\n",
      "Test F1-Score: 0.324431\n",
      "\n",
      "Epoch: 49/100\n",
      "\n",
      "Epoch: 49\n",
      "Train F1-Score: 0.370023\n",
      "Train Loss: 0.685920\n",
      "Validation F1-Score: 0.329382\n",
      "Validation Loss: 1.014107\n",
      "Test F1-Score: 0.347631\n",
      "\n",
      "Epoch: 50/100\n",
      "\n",
      "Epoch: 50\n",
      "Train F1-Score: 0.354452\n",
      "Train Loss: 0.660626\n",
      "Validation F1-Score: 0.323881\n",
      "Validation Loss: 0.989346\n",
      "Test F1-Score: 0.352402\n",
      "\n",
      "Epoch: 51/100\n",
      "\n",
      "Epoch: 51\n",
      "Train F1-Score: 0.392508\n",
      "Train Loss: 0.649678\n",
      "Validation F1-Score: 0.358774\n",
      "Validation Loss: 0.895655\n",
      "Test F1-Score: 0.321661\n",
      "\n",
      "Epoch: 52/100\n",
      "\n",
      "Epoch: 52\n",
      "Train F1-Score: 0.382574\n",
      "Train Loss: 0.685294\n",
      "Validation F1-Score: 0.387511\n",
      "Validation Loss: 1.014136\n",
      "Test F1-Score: 0.362185\n",
      "\n",
      "Epoch: 53/100\n",
      "\n",
      "Epoch: 53\n",
      "Train F1-Score: 0.449707\n",
      "Train Loss: 0.604965\n",
      "Validation F1-Score: 0.294643\n",
      "Validation Loss: 1.012665\n",
      "Test F1-Score: 0.296564\n",
      "\n",
      "Epoch: 54/100\n",
      "\n",
      "Epoch: 54\n",
      "Train F1-Score: 0.378386\n",
      "Train Loss: 0.635598\n",
      "Validation F1-Score: 0.374910\n",
      "Validation Loss: 1.044855\n",
      "Test F1-Score: 0.344879\n",
      "\n",
      "Epoch: 55/100\n",
      "\n",
      "Epoch: 55\n",
      "Train F1-Score: 0.416945\n",
      "Train Loss: 0.674691\n",
      "Validation F1-Score: 0.392208\n",
      "Validation Loss: 1.021346\n",
      "Test F1-Score: 0.344456\n",
      "\n",
      "Epoch: 56/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/greca/HD/anaconda3/envs/ser/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/media/greca/HD/anaconda3/envs/ser/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/media/greca/HD/anaconda3/envs/ser/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 56\n",
      "Train F1-Score: 0.377545\n",
      "Train Loss: 0.679462\n",
      "Validation F1-Score: 0.294643\n",
      "Validation Loss: 0.974003\n",
      "Test F1-Score: 0.296029\n",
      "\n",
      "Epoch: 57/100\n",
      "\n",
      "Epoch: 57\n",
      "Train F1-Score: 0.343606\n",
      "Train Loss: 0.669379\n",
      "Validation F1-Score: 0.445000\n",
      "Validation Loss: 0.968504\n",
      "Test F1-Score: 0.351615\n",
      "\n",
      "Epoch: 58/100\n",
      "\n",
      "Epoch: 58\n",
      "Train F1-Score: 0.371233\n",
      "Train Loss: 0.665736\n",
      "Validation F1-Score: 0.362293\n",
      "Validation Loss: 0.914923\n",
      "Test F1-Score: 0.296564\n",
      "\n",
      "Epoch: 59/100\n",
      "\n",
      "Epoch: 59\n",
      "Train F1-Score: 0.396108\n",
      "Train Loss: 0.632827\n",
      "Validation F1-Score: 0.295626\n",
      "Validation Loss: 1.028401\n",
      "Test F1-Score: 0.329490\n",
      "\n",
      "Epoch: 60/100\n",
      "\n",
      "Epoch: 60\n",
      "Train F1-Score: 0.372337\n",
      "Train Loss: 0.665757\n",
      "Validation F1-Score: 0.389235\n",
      "Validation Loss: 0.858317\n",
      "Test F1-Score: 0.294545\n",
      "\n",
      "Epoch: 61/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/greca/HD/anaconda3/envs/ser/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/media/greca/HD/anaconda3/envs/ser/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/media/greca/HD/anaconda3/envs/ser/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 61\n",
      "Train F1-Score: 0.386173\n",
      "Train Loss: 0.649314\n",
      "Validation F1-Score: 0.351549\n",
      "Validation Loss: 1.097445\n",
      "Test F1-Score: 0.359331\n",
      "\n",
      "Epoch: 62/100\n",
      "\n",
      "Epoch: 62\n",
      "Train F1-Score: 0.385333\n",
      "Train Loss: 0.630632\n",
      "Validation F1-Score: 0.390650\n",
      "Validation Loss: 0.870083\n",
      "Test F1-Score: 0.328228\n",
      "\n",
      "Epoch: 63/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/greca/HD/anaconda3/envs/ser/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/media/greca/HD/anaconda3/envs/ser/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/media/greca/HD/anaconda3/envs/ser/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 63\n",
      "Train F1-Score: 0.397671\n",
      "Train Loss: 0.616839\n",
      "Validation F1-Score: 0.291291\n",
      "Validation Loss: 1.096288\n",
      "Test F1-Score: 0.315530\n",
      "\n",
      "Epoch: 64/100\n",
      "\n",
      "Epoch: 64\n",
      "Train F1-Score: 0.388541\n",
      "Train Loss: 0.612763\n",
      "Validation F1-Score: 0.351549\n",
      "Validation Loss: 0.988618\n",
      "Test F1-Score: 0.360195\n",
      "\n",
      "Epoch: 65/100\n",
      "\n",
      "Epoch: 65\n",
      "Train F1-Score: 0.383311\n",
      "Train Loss: 0.638318\n",
      "Validation F1-Score: 0.388485\n",
      "Validation Loss: 1.131001\n",
      "Test F1-Score: 0.397373\n",
      "\n",
      "Epoch: 66/100\n",
      "\n",
      "Epoch: 66\n",
      "Train F1-Score: 0.381014\n",
      "Train Loss: 0.635248\n",
      "Validation F1-Score: 0.360462\n",
      "Validation Loss: 0.889018\n",
      "Test F1-Score: 0.333973\n",
      "\n",
      "Epoch: 67/100\n",
      "\n",
      "Epoch: 67\n",
      "Train F1-Score: 0.396950\n",
      "Train Loss: 0.626865\n",
      "Validation F1-Score: 0.352844\n",
      "Validation Loss: 0.921117\n",
      "Test F1-Score: 0.369532\n",
      "\n",
      "Epoch: 68/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/greca/HD/anaconda3/envs/ser/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/media/greca/HD/anaconda3/envs/ser/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/media/greca/HD/anaconda3/envs/ser/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 68\n",
      "Train F1-Score: 0.391065\n",
      "Train Loss: 0.603644\n",
      "Validation F1-Score: 0.360961\n",
      "Validation Loss: 0.996103\n",
      "Test F1-Score: 0.320585\n",
      "\n",
      "Epoch: 69/100\n",
      "\n",
      "Epoch: 69\n",
      "Train F1-Score: 0.380248\n",
      "Train Loss: 0.611115\n",
      "Validation F1-Score: 0.523829\n",
      "Validation Loss: 0.825975\n",
      "Test F1-Score: 0.360261\n",
      "\n",
      "Epoch: 70/100\n",
      "\n",
      "Epoch: 70\n",
      "Train F1-Score: 0.390864\n",
      "Train Loss: 0.636605\n",
      "Validation F1-Score: 0.330303\n",
      "Validation Loss: 0.945265\n",
      "Test F1-Score: 0.361286\n",
      "\n",
      "Epoch: 71/100\n",
      "\n",
      "Epoch: 71\n",
      "Train F1-Score: 0.382622\n",
      "Train Loss: 0.601570\n",
      "Validation F1-Score: 0.292609\n",
      "Validation Loss: 0.937060\n",
      "Test F1-Score: 0.339503\n",
      "\n",
      "Epoch: 72/100\n",
      "\n",
      "Epoch: 72\n",
      "Train F1-Score: 0.377077\n",
      "Train Loss: 0.675140\n",
      "Validation F1-Score: 0.481340\n",
      "Validation Loss: 0.757064\n",
      "Test F1-Score: 0.369532\n",
      "\n",
      "Epoch: 73/100\n",
      "\n",
      "Epoch: 73\n",
      "Train F1-Score: 0.376390\n",
      "Train Loss: 0.635483\n",
      "Validation F1-Score: 0.340370\n",
      "Validation Loss: 0.986767\n",
      "Test F1-Score: 0.357197\n",
      "\n",
      "Epoch: 74/100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 95\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, model_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 95\u001b[0m     train_f1, train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmixer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmixer\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m     valid_f1, valid_loss \u001b[38;5;241m=\u001b[39m evaluate(\n\u001b[1;32m    105\u001b[0m         device\u001b[38;5;241m=\u001b[39mdevice,\n\u001b[1;32m    106\u001b[0m         dataloader\u001b[38;5;241m=\u001b[39mvalidation_dataloader,\n\u001b[1;32m    107\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    108\u001b[0m         loss\u001b[38;5;241m=\u001b[39mloss\n\u001b[1;32m    109\u001b[0m     )\n\u001b[1;32m    111\u001b[0m     report \u001b[38;5;241m=\u001b[39m test(\n\u001b[1;32m    112\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    113\u001b[0m         dataloader\u001b[38;5;241m=\u001b[39mtest_dataloader,\n\u001b[1;32m    114\u001b[0m         device\u001b[38;5;241m=\u001b[39mdevice\n\u001b[1;32m    115\u001b[0m     )\n",
      "Cell \u001b[0;32mIn[2], line 41\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, dataloader, optimizer, loss, device, mixer)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mixer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     36\u001b[0m     data, target \u001b[38;5;241m=\u001b[39m mixer(\n\u001b[1;32m     37\u001b[0m         x\u001b[38;5;241m=\u001b[39mdata,\n\u001b[1;32m     38\u001b[0m         y\u001b[38;5;241m=\u001b[39mtarget\n\u001b[1;32m     39\u001b[0m     )\n\u001b[0;32m---> 41\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m l \u001b[38;5;241m=\u001b[39m loss(output, target)\n\u001b[1;32m     44\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m l\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m/media/greca/HD/anaconda3/envs/ser/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/media/greca/HD/GitHub/ser-wavelet/src/models/cnn3.py:291\u001b[0m, in \u001b[0;36mTransfer_CNN6.forward\u001b[0;34m(self, input, mixup_lambda)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, mixup_lambda\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 291\u001b[0m     output_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmixup_lambda\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m     embedding \u001b[38;5;241m=\u001b[39m output_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124membedding\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    293\u001b[0m     clipwise_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc_transfer(embedding)\n",
      "File \u001b[0;32m/media/greca/HD/anaconda3/envs/ser/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/media/greca/HD/GitHub/ser-wavelet/src/models/cnn3.py:230\u001b[0m, in \u001b[0;36mCnn6.forward\u001b[0;34m(self, input, mixup_lambda)\u001b[0m\n\u001b[1;32m    228\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_block1(\u001b[38;5;28minput\u001b[39m, pool_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m), pool_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mavg\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    229\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mdropout(x, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m)\n\u001b[0;32m--> 230\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv_block2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpool_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpool_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mavg\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mdropout(x, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m)\n\u001b[1;32m    232\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_block3(x, pool_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m), pool_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mavg\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/media/greca/HD/anaconda3/envs/ser/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/media/greca/HD/GitHub/ser-wavelet/src/models/cnn3.py:86\u001b[0m, in \u001b[0;36mConvBlock5x5.forward\u001b[0;34m(self, input, pool_size, pool_type)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, pool_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m), pool_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mavg\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     85\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\n\u001b[0;32m---> 86\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pool_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     88\u001b[0m         x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmax_pool2d(x, kernel_size\u001b[38;5;241m=\u001b[39mpool_size)\n",
      "File \u001b[0;32m/media/greca/HD/anaconda3/envs/ser/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/media/greca/HD/anaconda3/envs/ser/lib/python3.8/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/greca/HD/anaconda3/envs/ser/lib/python3.8/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(\n",
    "    params=model.parameters(),\n",
    "    lr=model_config[\"learning_rate\"],\n",
    "    betas=(0.9, 0.98),\n",
    "    eps=1e-9\n",
    ")\n",
    "noamopt = NoamOpt(512, 1, 400, optimizer)\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "scheduler = None\n",
    "mixer = None\n",
    "\n",
    "# creating the model checkpoint object\n",
    "sbm = SaveBestModel(\n",
    "    output_dir=os.path.join(model_config[\"output_path\"], dataset, mode, model_config[\"name\"]),\n",
    "    model_name=model_config[\"name\"]\n",
    ")\n",
    "\n",
    "if model_config[\"use_lr_scheduler\"]:\n",
    "    print(\"\\nWARNING: Using learning rate scheduler!\\n\")\n",
    "    scheduler = StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "if \"mixup\" in data_augmentation_config[\"techniques\"].keys():\n",
    "    print(\"\\nWARNING: Using mixup data augmentation technique!\\n\")\n",
    "    mixer = Mixup(\n",
    "        alpha=data_augmentation_config[\"techniques\"][\"mixup\"][\"alpha\"]\n",
    "    )\n",
    "\n",
    "if \"specmix\" in data_augmentation_config[\"techniques\"].keys():\n",
    "    print(\"\\nWARNING: Using specmix data augmentation technique!\\n\")\n",
    "    mixer = Specmix(\n",
    "        p=data_augmentation_config[\"p\"],\n",
    "        min_band_size=data_augmentation_config[\"techniques\"][\"specmix\"][\"min_band_size\"],\n",
    "        max_band_size=data_augmentation_config[\"techniques\"][\"specmix\"][\"max_band_size\"],\n",
    "        max_frequency_bands=data_augmentation_config[\"techniques\"][\"specmix\"][\"max_frequency_bands\"],\n",
    "        max_time_bands=data_augmentation_config[\"techniques\"][\"specmix\"][\"max_time_bands\"],\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "if \"cutmix\" in data_augmentation_config[\"techniques\"].keys():\n",
    "    print(\"\\nWARNING: Using cutmix data augmentation technique!\\n\")\n",
    "    mixer = Cutmix(\n",
    "        alpha=data_augmentation_config[\"techniques\"][\"cutmix\"][\"alpha\"],\n",
    "        p=data_augmentation_config[\"p\"]\n",
    "    )\n",
    "\n",
    "# creating the training dataloader\n",
    "training_dataloader = create_dataloader(\n",
    "    X=X_train,\n",
    "    y=y_train,\n",
    "    feature_config=feature_config,\n",
    "    wavelet_config=wavelet_config,\n",
    "    data_augmentation_config=data_augmentation_config,\n",
    "    num_workers=0,\n",
    "    mode=mode,\n",
    "    shuffle=False,\n",
    "    training=True,\n",
    "    batch_size=model_config[\"batch_size\"],\n",
    "    data_augment_target=data_augment_target\n",
    ")\n",
    "\n",
    "# creating the validation dataloader\n",
    "validation_dataloader = create_dataloader(\n",
    "    X=X_valid,\n",
    "    y=y_valid,\n",
    "    feature_config=feature_config,\n",
    "    wavelet_config=wavelet_config,\n",
    "    data_augmentation_config=data_augmentation_config,\n",
    "    num_workers=0,\n",
    "    mode=mode,\n",
    "    shuffle=False,\n",
    "    training=False,\n",
    "    batch_size=model_config[\"batch_size\"],\n",
    "    data_augment_target=data_augment_target\n",
    ")\n",
    "\n",
    "# creating the test dataloader\n",
    "test_dataloader = create_dataloader(\n",
    "    X=X_test,\n",
    "    y=y_test,\n",
    "    feature_config=feature_config,\n",
    "    wavelet_config=wavelet_config,\n",
    "    data_augmentation_config=None,\n",
    "    num_workers=0,\n",
    "    mode=params[\"mode\"],\n",
    "    shuffle=False,\n",
    "    training=False,\n",
    "    batch_size=params[\"model\"][\"batch_size\"],\n",
    "    data_augment_target=None\n",
    ")\n",
    "    \n",
    "# training loop\n",
    "for epoch in range(1, model_config[\"epochs\"] + 1):\n",
    "    print(f\"Epoch: {epoch}/{model_config['epochs']}\")\n",
    "\n",
    "    train_f1, train_loss = train(\n",
    "        device=device,\n",
    "        dataloader=training_dataloader,\n",
    "        optimizer=optimizer,\n",
    "        model=model,\n",
    "        loss=loss,\n",
    "        mixer=mixer\n",
    "    )\n",
    "\n",
    "    valid_f1, valid_loss = evaluate(\n",
    "        device=device,\n",
    "        dataloader=validation_dataloader,\n",
    "        model=model,\n",
    "        loss=loss\n",
    "    )\n",
    "    \n",
    "    report = test(\n",
    "        model=model,\n",
    "        dataloader=test_dataloader,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    test_f1 = report[\"macro avg\"][\"f1-score\"]\n",
    "\n",
    "    print(f\"\\nEpoch: {epoch}\")\n",
    "    print(f\"Train F1-Score: {train_f1:1.6f}\")\n",
    "    print(f\"Train Loss: {train_loss:1.6f}\")\n",
    "    print(f\"Validation F1-Score: {valid_f1:1.6f}\")\n",
    "    print(f\"Validation Loss: {valid_loss:1.6f}\")\n",
    "    print(f\"Test F1-Score: {test_f1:1.6f}\\n\")\n",
    "\n",
    "    # updating learning rate\n",
    "    if not scheduler is None:\n",
    "        scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5acbea2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
